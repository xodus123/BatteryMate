# ConvNeXt-Tiny Config (5클래스 다중 분류)
#
# ConvNeXt-Tiny: 28M params
# - 7x7 커널 사용 (넓은 receptive field)
# - Vision Transformer 아이디어를 CNN에 적용
# - Layer Normalization, GELU 활성화
# - 최신 아키텍처 (2022)
#
# 논문 권장 하이퍼파라미터:
# - AdamW optimizer, weight_decay 0.05
# - lr: 1e-4 ~ 3e-4 (fine-tuning)
# - Cosine decay with linear warmup (5 epochs)
# - drop_path_rate: 0.1 ~ 0.4
#
# 참고: timm 라이브러리 필요 (pip install timm)

# ============================================================
# 클래스 정의
# ============================================================
classes:
  names:
    - cell_normal
    - cell_porosity
    - module_normal
    - module_porosity
    - module_resin_overflow
  num_classes: 5
  class_weights: [1.5, 1.2, 0.8, 1.0, 8.0]

# ============================================================
# 모델 설정
# ============================================================
model:
  name: convnext_tiny       # timm 모델명
  backbone: timm            # timm 라이브러리 사용
  pretrained: true
  num_classes: 5
  dropout: 0.0              # ConvNeXt는 drop_path 사용
  drop_path_rate: 0.1       # Stochastic depth (ConvNeXt 권장)

# ============================================================
# 데이터 설정
# ============================================================
data:
  # Split 파일 경로 (원본 → 512 resize)
  train_split: training/data/splits/ct/resize512/battery_train.txt
  val_split: training/data/splits/ct/resize512/battery_val.txt
  test_split: training/data/splits/ct/resize512/battery_test.txt

  # 이미지 설정
  image_size: 512
  batch_size: 16            # ConvNeXt 메모리 고려
  num_workers: 4

  # 전처리 옵션
  preprocessed: true        # 전처리된 512 이미지 사용
  use_albumentations: false

  class_balancing:
    enabled: true
    method: weighted_sampler

  augmentation:
    train:
      - RandomHorizontalFlip:
          p: 0.5
      - RandomVerticalFlip:
          p: 0.5
      - RandomRotation:
          degrees: 90
      - RandomAffine:
          degrees: 0
          translate: [0.1, 0.1]
          scale: [0.9, 1.1]
    val: []

# ============================================================
# 학습 설정
# ============================================================
training:
  optimizer: AdamW
  lr: 0.0002               # ConvNeXt 논문 권장 (1e-4 ~ 3e-4)
  weight_decay: 0.05       # ConvNeXt 논문 권장값
  epochs: 50
  device: cuda

  scheduler:
    name: CosineAnnealingLR   # Cosine decay
    T_max: 50                 # total epochs
    eta_min: 1e-6

  warmup:
    enabled: true
    epochs: 5                 # 논문 권장: 5 epochs warmup

  gradient_clip: 1.0
  amp: true

# ============================================================
# 손실 함수 및 평가 기준
# ============================================================
criteria:
  loss: CrossEntropyLoss
  use_class_weights: true
  label_smoothing: 0.1

  focal_loss:
    enabled: true
    gamma: 2.0
    alpha: null

  early_stopping:
    enabled: true
    monitor: val_f1_macro
    patience: 7
    min_delta: 0.001
    mode: max

# ============================================================
# 체크포인트 설정
# ============================================================
checkpoint:
  save_dir: models/ct_cnn/checkpoints
  save_best_by: val_f1_macro
  save_last: true
  save_top_k: 3

# ============================================================
# 로깅 설정
# ============================================================
logging:
  tensorboard:
    enabled: true
    log_dir: models/ct_cnn/logs
    log_confusion_matrix: true
    log_per_class_metrics: true

  train_log:
    enabled: true
    save_path: models/ct_cnn/logs/train_convnext.csv

# ============================================================
# 실험 메타데이터
# ============================================================
experiment:
  name: ct_convnext_tiny
  description: "ConvNeXt-Tiny: 28M params, 7x7 커널, Raw Resize 512, 논문 권장 하이퍼파라미터"
  seed: 42
