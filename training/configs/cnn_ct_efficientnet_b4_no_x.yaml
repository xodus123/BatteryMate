# EfficientNet-B4 Config (x축 제외 버전)
#
# 기반: cnn_ct_efficientnet.yaml (B0 → B4로 업그레이드)
# 변경:
#   1. B0(5.3M) → B4(19M) - 더 큰 모델로 표현력 증가
#   2. x축 단면 데이터 제거 (y/z축만 학습)
#
# EfficientNet-B4: 19M params
# - 입력 해상도 380 (원본), 여기서는 512 사용
# - Compound Scaling: depth=1.8, width=1.4, resolution=1.4
# - Dropout 0.4 (B0: 0.2)
#
# 이유: x축에서 결함이 라벨링되지 않아 shortcut 학습 발생

# ============================================================
# 클래스 정의
# ============================================================
classes:
  names:
    - cell_normal
    - cell_porosity
    - module_normal
    - module_porosity
    - module_resin_overflow
  num_classes: 5
  class_weights: [2.0, 0.8, 2.5, 0.5, 10.0]

# ============================================================
# 모델 설정
# ============================================================
model:
  name: efficientnet_b4    # timm 모델명 (19M params)
  backbone: timm           # timm 라이브러리 사용
  pretrained: true
  num_classes: 5
  dropout: 0.4             # B4 논문 권장 (B0: 0.2, B4: 0.4)
  drop_path_rate: 0.2      # Stochastic depth

# ============================================================
# 데이터 설정
# ============================================================
data:
  # Split 파일 경로 (x축 제외)
  train_split: training/data/splits/ct/resize512_no_x/battery_train.txt
  val_split: training/data/splits/ct/resize512_no_x/battery_val.txt
  test_split: training/data/splits/ct/resize512/battery_test.txt

  # 이미지 설정
  image_size: 512
  batch_size: 16            # B4는 B0보다 메모리 사용량 큼
  num_workers: 4

  # 전처리 옵션
  preprocessed: true        # 전처리된 512 이미지 사용
  use_albumentations: false

  class_balancing:
    enabled: true
    method: weighted_sampler

  augmentation:
    train:
      - RandomHorizontalFlip:
          p: 0.5
      - RandomVerticalFlip:
          p: 0.5
      - RandomRotation:
          degrees: 90
      - RandomAffine:
          degrees: 0
          translate: [0.1, 0.1]
          scale: [0.9, 1.1]
    val: []

# ============================================================
# 학습 설정
# ============================================================
training:
  optimizer: AdamW           # Adam → AdamW (더 나은 정규화)
  lr: 0.0003                 # B4 fine-tuning 적합 lr
  weight_decay: 0.01
  epochs: 100
  device: cuda

  scheduler:
    name: CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 1e-6

  gradient_clip: 1.0
  amp: true

# ============================================================
# 손실 함수 및 평가 기준
# ============================================================
criteria:
  loss: CrossEntropyLoss
  use_class_weights: true
  label_smoothing: 0.1

  focal_loss:
    enabled: true
    gamma: 2.0
    alpha: null

  early_stopping:
    enabled: true
    monitor: val_f1_macro
    patience: 7
    min_delta: 0.001
    mode: max

# ============================================================
# 체크포인트 설정
# ============================================================
checkpoint:
  save_dir: models/ct_cnn/checkpoints
  save_best_by: val_f1_macro
  save_last: true
  save_top_k: 3

# ============================================================
# 로깅 설정
# ============================================================
logging:
  tensorboard:
    enabled: true
    log_dir: models/ct_cnn/logs
    log_confusion_matrix: true
    log_per_class_metrics: true

  train_log:
    enabled: true
    save_path: models/ct_cnn/logs/train_efficientnet_b4_no_x.csv

# ============================================================
# 실험 메타데이터
# ============================================================
experiment:
  name: ct_efficientnet_b4_no_x
  description: "EfficientNet-B4 (19M params, x축 제외) - shortcut 학습 방지, y/z축만 학습"
  seed: 42
