# EfficientNet-B0 Config (5클래스 다중 분류)
#
# EfficientNet-B0: 5.3M params (ResNet18 11M의 절반)
# - Compound Scaling으로 효율적인 아키텍처
# - Squeeze-and-Excitation (SE) 블록 내장
# - 내장 Dropout 0.2 사용 (추가 dropout 불필요)
#
# 논문 권장 하이퍼파라미터 적용:
# - weight_decay: 1e-5 (매우 낮음)
# - optimizer: RMSprop (원본) 또는 Adam (fine-tuning)
# - lr: 0.001 (fine-tuning 시)
#
# 참고: timm 라이브러리 필요 (pip install timm)

# ============================================================
# 클래스 정의
# ============================================================
classes:
  names:
    - cell_normal
    - cell_porosity
    - module_normal
    - module_porosity
    - module_resin_overflow
  num_classes: 5
  class_weights: [1.5, 1.2, 0.8, 1.0, 8.0]

# ============================================================
# 모델 설정
# ============================================================
model:
  name: efficientnet_b0    # timm 모델명
  backbone: timm           # timm 라이브러리 사용
  pretrained: true
  num_classes: 5
  dropout: 0.2             # EfficientNet-B0 내장 기본값
  drop_path_rate: 0.2      # Stochastic depth (논문: survival prob 0.8)

# ============================================================
# 데이터 설정
# ============================================================
data:
  # Split 파일 경로 (원본 → 512 resize)
  train_split: training/data/splits/ct/resize512/battery_train.txt
  val_split: training/data/splits/ct/resize512/battery_val.txt
  test_split: training/data/splits/ct/resize512/battery_test.txt

  # 이미지 설정
  image_size: 512
  batch_size: 32
  num_workers: 4

  # 전처리 옵션
  preprocessed: true       # 전처리된 512 이미지 사용
  use_albumentations: false

  class_balancing:
    enabled: true
    method: weighted_sampler

  augmentation:
    train:
      - RandomHorizontalFlip:
          p: 0.5
      - RandomVerticalFlip:
          p: 0.5
      - RandomRotation:
          degrees: 90
      - RandomAffine:
          degrees: 0
          translate: [0.1, 0.1]
          scale: [0.9, 1.1]
    val: []

# ============================================================
# 학습 설정
# ============================================================
training:
  optimizer: Adam          # EfficientNet fine-tuning 권장
  lr: 0.001                # EfficientNet 논문 권장 (fine-tuning)
  weight_decay: 0.00001    # 1e-5: EfficientNet 논문 권장값
  epochs: 50
  device: cuda

  scheduler:
    name: ExponentialLR    # EfficientNet 원본: exponential decay
    gamma: 0.97            # 논문: decay 0.97 every 2.4 epochs

  gradient_clip: 1.0
  amp: true

# ============================================================
# 손실 함수 및 평가 기준
# ============================================================
criteria:
  loss: CrossEntropyLoss
  use_class_weights: true
  label_smoothing: 0.1

  focal_loss:
    enabled: true
    gamma: 2.0
    alpha: null

  early_stopping:
    enabled: true
    monitor: val_f1_macro
    patience: 7
    min_delta: 0.001
    mode: max

# ============================================================
# 체크포인트 설정
# ============================================================
checkpoint:
  save_dir: models/ct_cnn/checkpoints
  save_best_by: val_f1_macro
  save_last: true
  save_top_k: 3

# ============================================================
# 로깅 설정
# ============================================================
logging:
  tensorboard:
    enabled: true
    log_dir: models/ct_cnn/logs
    log_confusion_matrix: true
    log_per_class_metrics: true

  train_log:
    enabled: true
    save_path: models/ct_cnn/logs/train_efficientnet.csv

# ============================================================
# 실험 메타데이터
# ============================================================
experiment:
  name: ct_efficientnet_b0
  description: "EfficientNet-B0: 5.3M params, Raw Resize 512, 논문 권장 하이퍼파라미터 적용"
  seed: 42
