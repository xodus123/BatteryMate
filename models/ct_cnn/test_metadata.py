"""ResNet + Metadata Fusion í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)

í´ë˜ìŠ¤:
    0: cell_normal
    1: cell_porosity
    2: module_normal
    3: module_porosity
    4: module_resin_overflow
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
_project_root = Path(__file__).parent.parent.parent.absolute()
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np
from tqdm import tqdm
import argparse
import json
import csv
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import (
    f1_score, accuracy_score, precision_score, recall_score,
    confusion_matrix, classification_report, roc_auc_score
)

from models.ct_cnn.model_metadata import create_metadata_model
from training.configs.config_loader import ConfigLoader
from training.data.dataset_metadata import BatteryMetadataDataset
from training.data.transforms import build_transforms_from_config, get_transforms


class MetadataTester:
    """Metadata Fusion ëª¨ë¸ í…ŒìŠ¤í„°"""

    def __init__(self, checkpoint_path: str, config: dict = None, enable_tensorboard: bool = True):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            config: YAML config dict (Noneì´ë©´ checkpointì—ì„œ ë¡œë“œ)
            enable_tensorboard: TensorBoard ë¡œê¹… í™œì„±í™” ì—¬ë¶€
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.checkpoint_path = checkpoint_path
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

        # Config ë¡œë“œ (checkpoint ìš°ì„ , ì—†ìœ¼ë©´ ì™¸ë¶€ì—ì„œ ë°›ìŒ)
        self.config = checkpoint.get('config', config)
        if self.config is None:
            raise ValueError("Configë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í´ë˜ìŠ¤ ì •ë³´
        self.class_names = self.config.get('classes', {}).get('names',
            ['cell_normal', 'cell_porosity', 'module_normal', 'module_porosity', 'module_resin_overflow'])
        self.num_classes = len(self.class_names)

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model = create_metadata_model(
            num_classes=self.num_classes,
            pretrained=False,
            dropout=self.config['model'].get('dropout', 0.5)
        ).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        # Loss function
        self.criterion = nn.CrossEntropyLoss()

        # Test DataLoader
        self._create_test_dataloader()

        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = Path('models/ct_cnn/results')
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.log_dir = Path('models/ct_cnn/logs')
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # TensorBoard Writer ì„¤ì •
        self.writer = None
        self.tb_log_dir = None
        if enable_tensorboard:
            checkpoint_name = Path(checkpoint_path).stem
            self.tb_log_dir = self.log_dir / f'test_metadata_{checkpoint_name}_{self.timestamp}'
            self.tb_log_dir.mkdir(parents=True, exist_ok=True)
            self.writer = SummaryWriter(log_dir=str(self.tb_log_dir))
            print(f"âœ… TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬: {self.tb_log_dir}")

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - Epoch: {checkpoint.get('epoch', 'N/A')}")
        best_metric = checkpoint.get('best_metric', 'N/A')
        print(f"   - Best F1: {best_metric:.4f}" if isinstance(best_metric, float) else f"   - Best F1: {best_metric}")
        print(f"   - Num Classes: {self.num_classes}")
        print(f"   - Device: {self.device}")
        print(f"   - Test ë°ì´í„°: {len(self.test_dataset)}ê°œ\n")

    def _create_test_dataloader(self):
        """Test DataLoader ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
        config = self.config
        image_size = config['data']['image_size']
        batch_size = config['data']['batch_size']
        num_workers = config['data']['num_workers']
        preprocessed = config['data'].get('preprocessed', False)

        # Transform (TestëŠ” augmentation ì—†ìŒ)
        aug_config = config['data'].get('augmentation', None)
        if aug_config:
            test_transform = build_transforms_from_config(
                aug_config.get('val', []), 'ct', image_size, preprocessed
            )
        else:
            test_transform = get_transforms('ct', 'val', image_size, preprocessed)

        # Test Split ê²½ë¡œ
        test_split = config['data']['test_split']
        if not Path(test_split).is_absolute():
            test_split = str(_project_root / test_split)

        self.test_dataset = BatteryMetadataDataset(
            split_file=test_split,
            modality='ct',
            mode='test',
            transform=test_transform,
            image_size=image_size,
            preprocessed=preprocessed
        )

        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )

    def test(self) -> dict:
        """Test ë°ì´í„° í‰ê°€"""
        print(f"{'='*60}")
        print(f"Test ë°ì´í„° í‰ê°€ ì‹œì‘ (Metadata Fusion - 5í´ë˜ìŠ¤)")
        print(f"{'='*60}\n")

        total_loss = 0.0
        num_batches = 0

        all_labels = []
        all_preds = []
        all_probs = []

        with torch.no_grad():
            for batch_idx, (images, metadata, labels) in enumerate(tqdm(self.test_loader, desc="Testing")):
                images = images.to(self.device)
                metadata = metadata.to(self.device)
                labels_tensor = labels.to(self.device).long()

                # Forward (ì´ë¯¸ì§€ + ë©”íƒ€ë°ì´í„°)
                outputs = self.model(images, metadata)
                loss = self.criterion(outputs, labels_tensor)

                total_loss += loss.item()
                num_batches += 1

                # ì˜ˆì¸¡
                probs = torch.softmax(outputs, dim=1)
                preds = outputs.argmax(dim=1)

                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_probs.append(probs.cpu().numpy())

        avg_loss = total_loss / num_batches
        all_labels = np.array(all_labels)
        all_preds = np.array(all_preds)
        all_probs = np.vstack(all_probs)

        # Metrics ê³„ì‚°
        metrics = self._calculate_metrics(all_labels, all_preds, all_probs)
        metrics['loss'] = avg_loss

        # ê²°ê³¼ ì¶œë ¥
        self._print_results(metrics, all_labels, all_preds)

        # TensorBoard ë¡œê¹…
        if self.writer is not None:
            self._log_to_tensorboard(metrics, all_labels, all_preds, all_probs)

        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        self._save_results(metrics, all_labels, all_preds, all_probs)

        return {
            'metrics': metrics,
            'predictions': {
                'labels': all_labels,
                'preds': all_preds,
                'probs': all_probs
            }
        }

    def _calculate_metrics(self, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray) -> dict:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        accuracy = accuracy_score(labels, preds)
        f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
        f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
        precision_macro = precision_score(labels, preds, average='macro', zero_division=0)
        recall_macro = recall_score(labels, preds, average='macro', zero_division=0)

        f1_per_class = f1_score(labels, preds, average=None, zero_division=0)
        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)
        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)

        cm = confusion_matrix(labels, preds, labels=range(self.num_classes))

        try:
            roc_auc_ovr = roc_auc_score(labels, probs, multi_class='ovr', average='macro')
        except Exception:
            roc_auc_ovr = None

        class_counts = np.bincount(labels, minlength=self.num_classes)

        return {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'roc_auc_ovr': roc_auc_ovr,
            'f1_per_class': f1_per_class.tolist(),
            'precision_per_class': precision_per_class.tolist(),
            'recall_per_class': recall_per_class.tolist(),
            'confusion_matrix': cm.tolist(),
            'class_counts': class_counts.tolist(),
            'total_samples': len(labels)
        }

    def _print_results(self, metrics: dict, labels: np.ndarray, preds: np.ndarray):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"Test ê²°ê³¼ (Metadata Fusion)")
        print(f"{'='*60}")
        print(f"  Test Loss: {metrics['loss']:.4f}")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  F1 (macro): {metrics['f1_macro']:.4f}")
        print(f"  F1 (weighted): {metrics['f1_weighted']:.4f}")
        print(f"  Precision (macro): {metrics['precision_macro']:.4f}")
        print(f"  Recall (macro): {metrics['recall_macro']:.4f}")
        if metrics['roc_auc_ovr'] is not None:
            print(f"  ROC-AUC (OvR macro): {metrics['roc_auc_ovr']:.4f}")

        print(f"\n  í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print("-" * 60)
        report = classification_report(
            labels, preds,
            labels=list(range(len(self.class_names))),
            target_names=self.class_names,
            zero_division=0
        )
        print(report)

        print(f"\n  í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
        for i, (name, count) in enumerate(zip(self.class_names, metrics['class_counts'])):
            print(f"    {i}: {name}: {count}")

        print(f"{'='*60}\n")

    def _log_to_tensorboard(self, metrics: dict, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray):
        """TensorBoardì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        print("ğŸ“Š TensorBoard ë¡œê¹… ì¤‘...")

        # Scalars
        self.writer.add_scalar('Test/Loss', metrics['loss'], 0)
        self.writer.add_scalar('Test/Accuracy', metrics['accuracy'], 0)
        self.writer.add_scalar('Test/F1_macro', metrics['f1_macro'], 0)
        self.writer.add_scalar('Test/F1_weighted', metrics['f1_weighted'], 0)
        self.writer.add_scalar('Test/Precision_macro', metrics['precision_macro'], 0)
        self.writer.add_scalar('Test/Recall_macro', metrics['recall_macro'], 0)
        if metrics['roc_auc_ovr'] is not None:
            self.writer.add_scalar('Test/ROC_AUC_OvR', metrics['roc_auc_ovr'], 0)

        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        for i, class_name in enumerate(self.class_names):
            self.writer.add_scalar(f'Test/PerClass/F1/{class_name}', metrics['f1_per_class'][i], 0)
            self.writer.add_scalar(f'Test/PerClass/Precision/{class_name}', metrics['precision_per_class'][i], 0)
            self.writer.add_scalar(f'Test/PerClass/Recall/{class_name}', metrics['recall_per_class'][i], 0)

        # PR Curve
        for i, class_name in enumerate(self.class_names):
            binary_labels = (labels == i).astype(int)
            class_probs = probs[:, i]
            self.writer.add_pr_curve(f'Test/PR_Curve/{class_name}', binary_labels, class_probs, global_step=0)

        # Confusion Matrix
        cm = np.array(metrics['confusion_matrix'])
        fig, ax = plt.subplots(figsize=(12, 10))
        im = ax.imshow(cm, cmap='Blues', interpolation='nearest')
        ax.figure.colorbar(im, ax=ax)

        ax.set(
            xticks=np.arange(cm.shape[1]),
            yticks=np.arange(cm.shape[0]),
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ylabel='True Label',
            xlabel='Predicted Label',
            title='Confusion Matrix (Test Set) - Metadata Fusion'
        )
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black")

        fig.tight_layout()

        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', dpi=150)
        buf.seek(0)
        cm_image = Image.open(buf)
        cm_array = np.array(cm_image)
        self.writer.add_image('Test/Confusion_Matrix', cm_array, 0, dataformats='HWC')
        plt.close(fig)

        # Error Summary Table
        self._log_error_summary_table(cm)

        # í™•ë¥  íˆìŠ¤í† ê·¸ë¨
        for i, class_name in enumerate(self.class_names):
            class_probs = probs[:, i]
            self.writer.add_histogram(f'Test/Probabilities/{class_name}/all', class_probs, 0)

            true_mask = labels == i
            if true_mask.sum() > 0:
                self.writer.add_histogram(f'Test/Probabilities/{class_name}/true_samples', class_probs[true_mask], 0)

        # ì‹ ë¢°ë„ ë¶„í¬
        max_probs = probs.max(axis=1)
        correct_mask = preds == labels

        if correct_mask.sum() > 0:
            self.writer.add_histogram('Test/Confidence/correct', max_probs[correct_mask], 0)
        if (~correct_mask).sum() > 0:
            self.writer.add_histogram('Test/Confidence/incorrect', max_probs[~correct_mask], 0)

        # í´ë˜ìŠ¤ ë¶„í¬
        self._log_class_distribution(labels)

        self.writer.flush()
        print("âœ… TensorBoard ë¡œê¹… ì™„ë£Œ")

    def _log_error_summary_table(self, cm: np.ndarray):
        """ì—ëŸ¬ ìš”ì•½ í…Œì´ë¸”"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        try:
            data = []
            for i, class_name in enumerate(self.class_names):
                tp = cm[i, i]
                fn = cm[i, :].sum() - tp
                fp = cm[:, i].sum() - tp
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                data.append([class_name[:20], int(tp), int(fp), int(fn), f'{precision:.3f}', f'{recall:.3f}', f'{f1:.3f}'])

            fig, ax = plt.subplots(figsize=(14, 5))
            ax.axis('off')

            table = ax.table(
                cellText=data,
                colLabels=['Class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1'],
                cellLoc='center',
                loc='center',
                colWidths=[0.28, 0.1, 0.1, 0.1, 0.12, 0.12, 0.12]
            )
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 1.8)

            for j in range(7):
                table[(0, j)].set_facecolor('#4472C4')
                table[(0, j)].set_text_props(color='white', weight='bold')

            plt.title('Test Set - Classification Errors (Metadata Fusion)', fontsize=12, fontweight='bold', pad=20)
            plt.tight_layout()

            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            buf.seek(0)
            image = Image.open(buf)
            image_array = np.array(image)

            self.writer.add_image('Test/Error_Summary_Table', image_array, 0, dataformats='HWC')
            plt.close(fig)
        except Exception as e:
            print(f"âš ï¸ Error Summary Table ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _log_class_distribution(self, labels: np.ndarray):
        """í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        try:
            class_counts = np.bincount(labels, minlength=self.num_classes)

            fig, ax = plt.subplots(figsize=(12, 6))
            colors = plt.cm.tab10(np.linspace(0, 1, self.num_classes))
            bars = ax.bar(range(self.num_classes), class_counts, color=colors)

            ax.set_xlabel('Class')
            ax.set_ylabel('Count')
            ax.set_title('Test Set - Class Distribution (Metadata Fusion)')
            ax.set_xticks(range(self.num_classes))
            ax.set_xticklabels(self.class_names, rotation=45, ha='right')

            total = class_counts.sum()
            for bar, count in zip(bars, class_counts):
                height = bar.get_height()
                ax.annotate(f'{count:,}\n({count/total*100:.1f}%)',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom', fontsize=9)

            plt.tight_layout()

            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            buf.seek(0)
            image = Image.open(buf)
            image_array = np.array(image)

            self.writer.add_image('Test/Class_Distribution', image_array, 0, dataformats='HWC')
            plt.close(fig)
        except Exception as e:
            print(f"âš ï¸ Class Distribution ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _save_results(self, metrics: dict, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        checkpoint_name = Path(self.checkpoint_path).stem

        # JSON ì €ì¥
        json_path = self.results_dir / f'test_metadata_{checkpoint_name}_{self.timestamp}.json'

        json_data = {
            'model_type': 'metadata_fusion',
            'checkpoint': self.checkpoint_path,
            'timestamp': self.timestamp,
            'num_classes': self.num_classes,
            'class_names': self.class_names,
            'total_samples': metrics['total_samples'],
            'class_counts': dict(zip(self.class_names, metrics['class_counts'])),
            'metrics': {
                'loss': metrics['loss'],
                'accuracy': metrics['accuracy'],
                'f1_macro': metrics['f1_macro'],
                'f1_weighted': metrics['f1_weighted'],
                'precision_macro': metrics['precision_macro'],
                'recall_macro': metrics['recall_macro'],
                'roc_auc_ovr': metrics['roc_auc_ovr'],
            },
            'per_class_metrics': {
                name: {
                    'f1': metrics['f1_per_class'][i],
                    'precision': metrics['precision_per_class'][i],
                    'recall': metrics['recall_per_class'][i],
                    'support': metrics['class_counts'][i]
                }
                for i, name in enumerate(self.class_names)
            },
            'confusion_matrix': metrics['confusion_matrix']
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… JSON ê²°ê³¼ ì €ì¥: {json_path}")

        # CSV ì €ì¥
        csv_path = self.log_dir / f'test_metadata_{checkpoint_name}_{self.timestamp}.csv'

        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['metric', 'value'])
            writer.writerow(['model_type', 'metadata_fusion'])
            writer.writerow(['checkpoint', self.checkpoint_path])
            writer.writerow(['timestamp', self.timestamp])
            writer.writerow(['total_samples', metrics['total_samples']])
            writer.writerow(['loss', f"{metrics['loss']:.4f}"])
            writer.writerow(['accuracy', f"{metrics['accuracy']:.4f}"])
            writer.writerow(['f1_macro', f"{metrics['f1_macro']:.4f}"])
            writer.writerow(['f1_weighted', f"{metrics['f1_weighted']:.4f}"])
            writer.writerow(['precision_macro', f"{metrics['precision_macro']:.4f}"])
            writer.writerow(['recall_macro', f"{metrics['recall_macro']:.4f}"])
            if metrics['roc_auc_ovr'] is not None:
                writer.writerow(['roc_auc_ovr', f"{metrics['roc_auc_ovr']:.4f}"])

            writer.writerow([])
            writer.writerow(['--- Per Class Metrics ---', ''])

            for i, name in enumerate(self.class_names):
                writer.writerow([f'{name}_f1', f"{metrics['f1_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_precision', f"{metrics['precision_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_recall', f"{metrics['recall_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_support', metrics['class_counts'][i]])

        print(f"âœ… CSV ê²°ê³¼ ì €ì¥: {csv_path}")

        # Confusion Matrix ì´ë¯¸ì§€ ì €ì¥
        self._save_confusion_matrix_image(metrics['confusion_matrix'], checkpoint_name)

    def _save_confusion_matrix_image(self, cm: list, checkpoint_name: str):
        """Confusion Matrix ì´ë¯¸ì§€ ì €ì¥"""
        import matplotlib.pyplot as plt

        cm = np.array(cm)
        fig, ax = plt.subplots(figsize=(12, 10))
        im = ax.imshow(cm, cmap='Blues', interpolation='nearest')
        ax.figure.colorbar(im, ax=ax)

        ax.set(
            xticks=np.arange(cm.shape[1]),
            yticks=np.arange(cm.shape[0]),
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ylabel='True Label',
            xlabel='Predicted Label',
            title=f'Confusion Matrix (Metadata Fusion) - {checkpoint_name}'
        )
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black")

        fig.tight_layout()

        img_path = self.results_dir / f'confusion_matrix_metadata_{checkpoint_name}_{self.timestamp}.png'
        plt.savefig(img_path, dpi=150, bbox_inches='tight')
        plt.close(fig)
        print(f"âœ… Confusion Matrix ì´ë¯¸ì§€ ì €ì¥: {img_path}")

    def close(self):
        """TensorBoard Writer ë‹«ê¸°"""
        if self.writer is not None:
            self.writer.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Metadata Fusion Model Test')
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--config',
        type=str,
        default=None,
        help='Config íŒŒì¼ ê²½ë¡œ (ìƒëµí•˜ë©´ checkpointì—ì„œ ë¡œë“œ)'
    )
    parser.add_argument(
        '--no-tensorboard',
        action='store_true',
        help='TensorBoard ë¡œê¹… ë¹„í™œì„±í™”'
    )

    args = parser.parse_args()

    # Config ë¡œë“œ (ì„ íƒì‚¬í•­)
    config = None
    if args.config:
        config = ConfigLoader.load(args.config)

    # Tester ìƒì„± ë° í‰ê°€
    tester = MetadataTester(
        checkpoint_path=args.checkpoint,
        config=config,
        enable_tensorboard=not args.no_tensorboard
    )
    results = tester.test()

    # ì¶”ê°€ ë¶„ì„
    print("\nğŸ“Š ìƒì„¸ ë¶„ì„:")
    preds_data = results['predictions']
    labels = preds_data['labels']
    preds = preds_data['preds']
    probs = preds_data['probs']

    # í´ë˜ìŠ¤ë³„ ì˜¤ë‹µ ë¶„ì„
    print("\n  í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ë¶„ì„:")
    for i, class_name in enumerate(tester.class_names):
        fn_mask = (labels == i) & (preds != i)
        fn_count = fn_mask.sum()

        fp_mask = (labels != i) & (preds == i)
        fp_count = fp_mask.sum()

        print(f"    {class_name}:")
        print(f"      - FN (ë†“ì¹œ ê²ƒ): {fn_count}ê°œ")
        print(f"      - FP (ì˜ëª» ì˜ˆì¸¡): {fp_count}ê°œ")

    # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„
    max_probs = probs.max(axis=1)
    correct_mask = preds == labels

    print(f"\n  ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„:")
    print(f"    ì •ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„: {max_probs[correct_mask].mean():.4f} (std: {max_probs[correct_mask].std():.4f})")
    if (~correct_mask).sum() > 0:
        print(f"    ì˜¤ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„: {max_probs[~correct_mask].mean():.4f} (std: {max_probs[~correct_mask].std():.4f})")

    tester.close()

    print(f"\n{'='*60}")
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   - TensorBoard: tensorboard --logdir={tester.tb_log_dir}")
    print(f"   - ê²°ê³¼ íŒŒì¼: {tester.results_dir}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
