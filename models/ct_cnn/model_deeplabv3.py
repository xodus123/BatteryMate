"""DeepLabV3+ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ í™œìš© ë¶„ë¥˜ ëª¨ë¸

ì›ë³¸ DeepLabV3+ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸(DRN-D-54 + ASPP, 4í´ë˜ìŠ¤)ì˜
backbone + ASPP ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•˜ì—¬ ë¶„ë¥˜ íƒœìŠ¤í¬ì— ì ìš©.

í¬íŒ… ì¶œì²˜: D:\ëª¨ë¸\1.ëª¨ë¸ì†ŒìŠ¤ì½”ë“œ\ëª¨ë¸1_DeepLabv3\pytorch-deeplab-xception-eval\modeling\
  - backbone/drn.py â†’ Bottleneck, DRN í´ë˜ìŠ¤
  - aspp.py â†’ _ASPPModule, ASPP í´ë˜ìŠ¤
  - SynchronizedBatchNorm2d â†’ nn.BatchNorm2d êµì²´ (ê°€ì¤‘ì¹˜ í˜•ì‹ ë™ì¼)

ì•„í‚¤í…ì²˜:
  DRN-D-54 backbone (output_stride=8)
  â†’ ASPP (512ch â†’ 256ch, rates=[1,12,24,36])
  â†’ Classification head: GAP â†’ Dropout â†’ FC(256â†’5)

í•µì‹¬ ê°€ì¹˜: ë°°í„°ë¦¬ CT ì´ë¯¸ì§€ë¡œ í•™ìŠµëœ DRN-D-54 backboneì„ í™œìš©í•˜ë¯€ë¡œ,
ImageNet pretrainedë³´ë‹¤ ë‚˜ì€ feature extraction ê¸°ëŒ€.
"""
import math
import torch
import torch.nn as nn
import torch.nn.functional as F


# ============================================================
# DRN-D-54 Backbone (ì›ë³¸ drn.py í¬íŒ…)
# ============================================================

def conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=padding, bias=False, dilation=dilation)


class Bottleneck(nn.Module):
    """ì›ë³¸ drn.py Bottleneck ê·¸ëŒ€ë¡œ í¬íŒ… (state_dict í‚¤ í˜¸í™˜)"""
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None,
                 dilation=(1, 1), residual=True, BatchNorm=None):
        super().__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = BatchNorm(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=dilation[1], bias=False,
                               dilation=dilation[1])
        self.bn2 = BatchNorm(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = BatchNorm(planes * 4)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


class DRN(nn.Module):
    """ì›ë³¸ drn.py DRN í´ë˜ìŠ¤ (arch='D' ì „ìš©ìœ¼ë¡œ ê°„ì†Œí™”)

    DRN-D-54 êµ¬ì¡°:
      layer0: Conv7x7 stride=1 â†’ 16ch
      layer1: Conv3x3 â†’ 16ch
      layer2: Conv3x3 stride=2 â†’ 32ch
      layer3: 3Ã—Bottleneck stride=2 â†’ 256ch  â† low_level_feat
      layer4: 4Ã—Bottleneck stride=2 â†’ 512ch
      layer5: 6Ã—Bottleneck dilation=2 â†’ 1024ch
      layer6: 3Ã—Bottleneck dilation=4 â†’ 2048ch
      layer7: Conv3x3 dilation=2 â†’ 512ch
      layer8: Conv3x3 dilation=1 â†’ 512ch
    """

    def __init__(self, block, layers,
                 channels=(16, 32, 64, 128, 256, 512, 512, 512),
                 BatchNorm=None):
        super().__init__()
        self.inplanes = channels[0]
        self.out_dim = channels[-1]

        self.layer0 = nn.Sequential(
            nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3, bias=False),
            BatchNorm(channels[0]),
            nn.ReLU(inplace=True)
        )

        self.layer1 = self._make_conv_layers(
            channels[0], layers[0], stride=1, BatchNorm=BatchNorm)
        self.layer2 = self._make_conv_layers(
            channels[1], layers[1], stride=2, BatchNorm=BatchNorm)

        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2, BatchNorm=BatchNorm)
        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2, BatchNorm=BatchNorm)
        self.layer5 = self._make_layer(block, channels[4], layers[4],
                                       dilation=2, new_level=False, BatchNorm=BatchNorm)
        self.layer6 = None if layers[5] == 0 else \
            self._make_layer(block, channels[5], layers[5], dilation=4,
                             new_level=False, BatchNorm=BatchNorm)

        self.layer7 = None if layers[6] == 0 else \
            self._make_conv_layers(channels[6], layers[6], dilation=2, BatchNorm=BatchNorm)
        self.layer8 = None if layers[7] == 0 else \
            self._make_conv_layers(channels[7], layers[7], dilation=1, BatchNorm=BatchNorm)

        self._init_weight()

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,
                    new_level=True, residual=True, BatchNorm=None):
        assert dilation == 1 or dilation % 2 == 0
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                BatchNorm(planes * block.expansion),
            )

        layers = list()
        layers.append(block(
            self.inplanes, planes, stride, downsample,
            dilation=(1, 1) if dilation == 1 else (
                dilation // 2 if new_level else dilation, dilation),
            residual=residual, BatchNorm=BatchNorm))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, residual=residual,
                                dilation=(dilation, dilation), BatchNorm=BatchNorm))

        return nn.Sequential(*layers)

    def _make_conv_layers(self, channels, convs, stride=1, dilation=1, BatchNorm=None):
        modules = []
        for i in range(convs):
            modules.extend([
                nn.Conv2d(self.inplanes, channels, kernel_size=3,
                          stride=stride if i == 0 else 1,
                          padding=dilation, bias=False, dilation=dilation),
                BatchNorm(channels),
                nn.ReLU(inplace=True)])
            self.inplanes = channels
        return nn.Sequential(*modules)

    def forward(self, x):
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)

        x = self.layer3(x)
        low_level_feat = x

        x = self.layer4(x)
        x = self.layer5(x)

        if self.layer6 is not None:
            x = self.layer6(x)

        if self.layer7 is not None:
            x = self.layer7(x)

        if self.layer8 is not None:
            x = self.layer8(x)

        return x, low_level_feat


# ============================================================
# ASPP ëª¨ë“ˆ (ì›ë³¸ aspp.py í¬íŒ…)
# ============================================================

class _ASPPModule(nn.Module):
    """ì›ë³¸ aspp.py _ASPPModule ê·¸ëŒ€ë¡œ í¬íŒ… (ì¼ë°˜ Conv2d, depthwise ì•„ë‹˜)"""

    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):
        super().__init__()
        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,
                                     stride=1, padding=padding, dilation=dilation, bias=False)
        self.bn = BatchNorm(planes)
        self.relu = nn.ReLU()

        self._init_weight()

    def forward(self, x):
        x = self.atrous_conv(x)
        x = self.bn(x)
        return self.relu(x)

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()


class ASPP(nn.Module):
    """ì›ë³¸ aspp.py ASPP êµ¬ì¡° ê·¸ëŒ€ë¡œ í¬íŒ…

    DRN backboneìš© (inplanes=512, output_stride=8):
      aspp1: 1x1 conv (dilation=1) â†’ 256ch
      aspp2: 3x3 conv (dilation=12) â†’ 256ch
      aspp3: 3x3 conv (dilation=24) â†’ 256ch
      aspp4: 3x3 conv (dilation=36) â†’ 256ch
      global_avg_pool: GAP â†’ 1x1 conv â†’ 256ch
      â†’ concat 1280ch â†’ 1x1 conv â†’ 256ch
    """

    def __init__(self, backbone, output_stride, BatchNorm):
        super().__init__()
        if backbone == 'drn':
            inplanes = 512
        else:
            inplanes = 2048

        if output_stride == 16:
            dilations = [1, 6, 12, 18]
        elif output_stride == 8:
            dilations = [1, 12, 24, 36]
        else:
            raise NotImplementedError

        self.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)
        self.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)
        self.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)
        self.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)

        self.global_avg_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Conv2d(inplanes, 256, 1, stride=1, bias=False),
            BatchNorm(256),
            nn.ReLU()
        )
        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)
        self.bn1 = BatchNorm(256)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self._init_weight()

    def forward(self, x):
        x1 = self.aspp1(x)
        x2 = self.aspp2(x)
        x3 = self.aspp3(x)
        x4 = self.aspp4(x)
        x5 = self.global_avg_pool(x)
        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)
        x = torch.cat((x1, x2, x3, x4, x5), dim=1)

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        return self.dropout(x)

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()


# ============================================================
# DeepLabV3 ë¶„ë¥˜ ëª¨ë¸
# ============================================================

class DeepLabV3Classifier(nn.Module):
    """DeepLabV3+ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ í™œìš© ë¶„ë¥˜ ëª¨ë¸

    ì›ë³¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì˜ backbone(DRN-D-54) + ASPP ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³ ,
    decoderë¥¼ GAP + FC ë¶„ë¥˜ í—¤ë“œë¡œ êµì²´.

    state_dict í‚¤ ë§¤í•‘:
      self.backbone â†’ ì›ë³¸ 'backbone.*' í‚¤
      self.aspp â†’ ì›ë³¸ 'aspp.*' í‚¤
      self.classifier â†’ ìƒˆë¡œ í•™ìŠµ (decoder í‚¤ ìŠ¤í‚µ)
    """

    def __init__(self, num_classes: int = 5, dropout: float = 0.5,
                 freeze_backbone: bool = True,
                 pretrained_segmentation: str = None):
        super().__init__()

        BatchNorm = nn.BatchNorm2d

        # DRN-D-54 backbone (ì›ë³¸ê³¼ ë™ì¼ êµ¬ì¡°)
        self.backbone = DRN(
            block=Bottleneck,
            layers=[1, 1, 3, 4, 6, 3, 1, 1],
            channels=(16, 32, 64, 128, 256, 512, 512, 512),
            BatchNorm=BatchNorm
        )

        # ASPP (DRN backbone, output_stride=8)
        self.aspp = ASPP(backbone='drn', output_stride=8, BatchNorm=BatchNorm)

        # ë¶„ë¥˜ í—¤ë“œ (ì›ë³¸ decoder ëŒ€ì²´)
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Dropout(p=dropout),
            nn.Linear(256, num_classes)
        )

        # ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if pretrained_segmentation:
            self.load_pretrained_segmentation(pretrained_segmentation)

        # backbone + ASPP freeze
        if freeze_backbone:
            self._freeze_backbone_aspp()

        self._print_info(num_classes, dropout, freeze_backbone, pretrained_segmentation)

    def load_pretrained_segmentation(self, checkpoint_path: str):
        """ì›ë³¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì—ì„œ backbone + ASPP ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ (decoder ìŠ¤í‚µ)"""
        print(f"ğŸ“¦ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint

        # backbone + aspp í‚¤ë§Œ í•„í„°ë§ (decoder í‚¤ ì œì™¸)
        loaded_keys = []
        skipped_keys = []
        for key, value in state_dict.items():
            if key.startswith('backbone.') or key.startswith('aspp.'):
                loaded_keys.append(key)
            else:
                skipped_keys.append(key)

        filtered_state_dict = {k: v for k, v in state_dict.items() if k in loaded_keys}

        # strict=False: classifier í‚¤ëŠ” ì²´í¬í¬ì¸íŠ¸ì— ì—†ìœ¼ë¯€ë¡œ
        missing, unexpected = self.load_state_dict(filtered_state_dict, strict=False)

        # classifier í‚¤ë§Œ missingì´ì–´ì•¼ ì •ìƒ
        classifier_missing = [k for k in missing if k.startswith('classifier.')]
        other_missing = [k for k in missing if not k.startswith('classifier.')]

        print(f"  âœ… ë¡œë“œëœ í‚¤: {len(loaded_keys)}ê°œ (backbone + aspp)")
        print(f"  â­ï¸  ìŠ¤í‚µëœ í‚¤: {len(skipped_keys)}ê°œ (decoder)")
        if other_missing:
            print(f"  âš ï¸  ë§¤ì¹­ ì‹¤íŒ¨: {other_missing}")
        print(f"  ğŸ†• ìƒˆë¡œ í•™ìŠµí•  í‚¤: {len(classifier_missing)}ê°œ (classifier)")

    def _freeze_backbone_aspp(self):
        """backbone + ASPP íŒŒë¼ë¯¸í„° freeze + eval ëª¨ë“œ ê³ ì •"""
        for param in self.backbone.parameters():
            param.requires_grad = False
        for param in self.aspp.parameters():
            param.requires_grad = False
        self.backbone.eval()
        self.aspp.eval()
        self._frozen = True

    def train(self, mode=True):
        """frozen ëª¨ë“ˆì€ í•­ìƒ eval ëª¨ë“œ ìœ ì§€ (BatchNorm ì•ˆì •ì„±)"""
        super().train(mode)
        if getattr(self, '_frozen', False) and mode:
            self.backbone.eval()
            self.aspp.eval()
        return self

    def _print_info(self, num_classes, dropout, freeze_backbone, pretrained_segmentation):
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.parameters())
        print(f"âœ… DeepLabV3 ë¶„ë¥˜ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        print(f"   - Backbone: DRN-D-54 (output_stride=8)")
        print(f"   - ASPP: rates=[1,12,24,36]")
        print(f"   - Num classes: {num_classes}")
        print(f"   - Dropout: {dropout}")
        print(f"   - Freeze backbone+ASPP: {freeze_backbone}")
        print(f"   - Pretrained: {'ì„¸ê·¸ë©˜í…Œì´ì…˜ ê°€ì¤‘ì¹˜' if pretrained_segmentation else 'ì—†ìŒ'}")
        print(f"   - Parameters: {trainable / 1e6:.2f}M trainable / {total / 1e6:.2f}M total")

    def forward(self, x):
        """
        Args:
            x: (B, 3, H, W)
        Returns:
            logits: (B, num_classes)
        """
        x, _ = self.backbone(x)  # low_level_feat ì‚¬ìš© ì•ˆ í•¨
        x = self.aspp(x)         # (B, 256, H/8, W/8)
        return self.classifier(x)


def create_deeplabv3_model(config: dict) -> nn.Module:
    """Config ê¸°ë°˜ DeepLabV3 ë¶„ë¥˜ ëª¨ë¸ ìƒì„±"""
    model_cfg = config['model']
    return DeepLabV3Classifier(
        num_classes=model_cfg.get('num_classes', 5),
        dropout=model_cfg.get('dropout', 0.5),
        freeze_backbone=model_cfg.get('freeze_backbone', True),
        pretrained_segmentation=model_cfg.get('pretrained_segmentation', None),
    )


# ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import os

    print("=" * 60)
    print("DeepLabV3 ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # 1. ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ì—†ì´ ìƒì„±
    print("\n[1] ê¸°ë³¸ ëª¨ë¸ ìƒì„± (ê°€ì¤‘ì¹˜ ì—†ìŒ)")
    model = DeepLabV3Classifier(num_classes=5, freeze_backbone=False)

    dummy = torch.randn(2, 3, 512, 512)
    out = model(dummy)
    print(f"  Output shape: {out.shape}")  # (2, 5)
    assert out.shape == (2, 5), f"Expected (2, 5), got {out.shape}"

    # 2. ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ í…ŒìŠ¤íŠ¸
    ckpt_path = "models/ct_cnn/checkpoints/deeplabv3_drn_ct.pt"
    if os.path.exists(ckpt_path):
        print(f"\n[2] ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ í…ŒìŠ¤íŠ¸")
        model2 = DeepLabV3Classifier(
            num_classes=5,
            freeze_backbone=True,
            pretrained_segmentation=ckpt_path
        )
        out2 = model2(dummy)
        print(f"  Output shape: {out2.shape}")
        assert out2.shape == (2, 5)
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        print(f"\nâš ï¸  ì²´í¬í¬ì¸íŠ¸ ë¯¸ë°œê²¬: {ckpt_path}")
        print("  ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë§Œ í†µê³¼")
