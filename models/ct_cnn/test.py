"""CT CNN í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)

í´ë˜ìŠ¤:
    0: cell_normal
    1: cell_porosity
    2: module_normal
    3: module_porosity
    4: module_resin_overflow
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
_project_root = Path(__file__).parent.parent.parent.absolute()
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import torch
import torch.nn as nn
from pathlib import Path
import numpy as np
from tqdm import tqdm
import argparse
import json
import csv
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import (
    f1_score, accuracy_score, precision_score, recall_score,
    confusion_matrix, classification_report, roc_auc_score
)

from models.ct_cnn.model import create_model
from training.configs.config_loader import ConfigLoader
from training.data.dataloader import create_test_dataloader
from training.visualization.tensorboard_logger import TensorBoardLogger


class CNNTester:
    """CNN í…ŒìŠ¤í„° (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)"""

    def __init__(self, checkpoint_path: str, config: dict = None, enable_tensorboard: bool = True):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            config: YAML config dict (Noneì´ë©´ checkpointì—ì„œ ë¡œë“œ)
            enable_tensorboard: TensorBoard ë¡œê¹… í™œì„±í™” ì—¬ë¶€
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.checkpoint_path = checkpoint_path
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

        # Config ë¡œë“œ (checkpoint ìš°ì„ , ì—†ìœ¼ë©´ ì™¸ë¶€ì—ì„œ ë°›ìŒ)
        self.config = checkpoint.get('config', config)
        if self.config is None:
            raise ValueError("Configë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. checkpointì— configê°€ ì—†ê±°ë‚˜ ì™¸ë¶€ configë¥¼ ì œê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # í´ë˜ìŠ¤ ì •ë³´
        self.class_names = self.config.get('classes', {}).get('names',
            ['cell_normal', 'cell_porosity', 'module_normal', 'module_porosity', 'module_resin_overflow'])
        self.num_classes = len(self.class_names)

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model = create_model(self.config).to(self.device)

        # ì´ì „ ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„± ì²˜ë¦¬ (01-20 ì´ì „: model.fc.weight â†’ model.fc.1.weight)
        state_dict = checkpoint['model_state_dict']
        if 'model.fc.weight' in state_dict and 'model.fc.1.weight' not in state_dict:
            print("âš ï¸ êµ¬ í˜•ì‹ ì²´í¬í¬ì¸íŠ¸ ê°ì§€ â†’ state_dict í‚¤ ë³€í™˜ (model.fc â†’ model.fc.1)")
            state_dict['model.fc.1.weight'] = state_dict.pop('model.fc.weight')
            state_dict['model.fc.1.bias'] = state_dict.pop('model.fc.bias')

        self.model.load_state_dict(state_dict)
        self.model.eval()

        # Loss function (ë‹¤ì¤‘ë¶„ë¥˜)
        self.criterion = nn.CrossEntropyLoss()

        # Test DataLoader
        # Augmentation config ê°€ì ¸ì˜¤ê¸° (TestëŠ” augmentation ì—†ì´ ì‚¬ìš©)
        augmentation_config = self.config['data'].get('augmentation', None)

        self.test_loader = create_test_dataloader(
            test_split_file=self.config['data']['test_split'],
            batch_size=self.config['data']['batch_size'],
            num_workers=self.config['data']['num_workers'],
            image_size=self.config['data']['image_size'],
            modality='ct',
            preprocessed=self.config['data'].get('preprocessed', False),
            use_albumentations=self.config['data'].get('use_albumentations', False),
            augmentation_config=augmentation_config
        )

        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = Path('models/ct_cnn/results')
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.log_dir = Path('models/ct_cnn/logs')
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # TensorBoard Writer ì„¤ì •
        self.writer = None
        self.tb_log_dir = None
        if enable_tensorboard:
            checkpoint_name = Path(checkpoint_path).stem
            self.tb_log_dir = self.log_dir / f'test_{checkpoint_name}_{self.timestamp}'
            self.tb_log_dir.mkdir(parents=True, exist_ok=True)
            self.writer = SummaryWriter(log_dir=str(self.tb_log_dir))
            print(f"âœ… TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬: {self.tb_log_dir}")

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - Epoch: {checkpoint.get('epoch', 'N/A')}")
        val_f1 = checkpoint.get('val_f1_macro', checkpoint.get('val_f1', 'N/A'))
        print(f"   - Val F1: {val_f1:.4f}" if isinstance(val_f1, float) else f"   - Val F1: {val_f1}")
        print(f"   - Num Classes: {self.num_classes}")
        print(f"   - Classes: {self.class_names}")
        print(f"   - Device: {self.device}")
        print(f"   - Test ë°ì´í„°: {len(self.test_loader.dataset)}ê°œ\n")

    def test(self) -> dict:
        """Test ë°ì´í„° í‰ê°€"""
        print(f"{'='*60}")
        print(f"Test ë°ì´í„° í‰ê°€ ì‹œì‘ (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)")
        print(f"{'='*60}\n")

        total_loss = 0.0
        num_batches = 0

        all_labels = []
        all_preds = []
        all_probs = []

        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(tqdm(self.test_loader, desc="Testing")):
                images = images.to(self.device)
                labels_tensor = labels.to(self.device).long()

                # Forward
                outputs = self.model(images)  # (B, num_classes)
                loss = self.criterion(outputs, labels_tensor)

                total_loss += loss.item()
                num_batches += 1

                # ì˜ˆì¸¡ (Softmax + Argmax)
                probs = torch.softmax(outputs, dim=1)  # (B, num_classes)
                preds = outputs.argmax(dim=1)  # (B,)

                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_probs.append(probs.cpu().numpy())

        avg_loss = total_loss / num_batches
        all_labels = np.array(all_labels)
        all_preds = np.array(all_preds)
        all_probs = np.vstack(all_probs)  # (N, num_classes)

        # Metrics ê³„ì‚°
        metrics = self._calculate_metrics(all_labels, all_preds, all_probs)
        metrics['loss'] = avg_loss

        # ê²°ê³¼ ì¶œë ¥
        self._print_results(metrics, all_labels, all_preds)

        # TensorBoard ë¡œê¹…
        if self.writer is not None:
            self._log_to_tensorboard(metrics, all_labels, all_preds, all_probs)

        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        self._save_results(metrics, all_labels, all_preds, all_probs)

        return {
            'metrics': metrics,
            'predictions': {
                'labels': all_labels,
                'preds': all_preds,
                'probs': all_probs
            }
        }

    def _calculate_metrics(self, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray) -> dict:
        """ë©”íŠ¸ë¦­ ê³„ì‚° (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)"""
        # ì „ì²´ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(labels, preds)
        f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
        f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
        precision_macro = precision_score(labels, preds, average='macro', zero_division=0)
        recall_macro = recall_score(labels, preds, average='macro', zero_division=0)

        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        f1_per_class = f1_score(labels, preds, average=None, zero_division=0)
        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)
        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)

        # Confusion Matrix
        cm = confusion_matrix(labels, preds, labels=range(self.num_classes))

        # ROC-AUC (One-vs-Rest, macro)
        try:
            roc_auc_ovr = roc_auc_score(labels, probs, multi_class='ovr', average='macro')
        except Exception:
            roc_auc_ovr = None

        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜
        class_counts = np.bincount(labels, minlength=self.num_classes)

        return {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'roc_auc_ovr': roc_auc_ovr,
            'f1_per_class': f1_per_class.tolist(),
            'precision_per_class': precision_per_class.tolist(),
            'recall_per_class': recall_per_class.tolist(),
            'confusion_matrix': cm.tolist(),
            'class_counts': class_counts.tolist(),
            'total_samples': len(labels)
        }

    def _print_results(self, metrics: dict, labels: np.ndarray, preds: np.ndarray):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"Test ê²°ê³¼")
        print(f"{'='*60}")
        print(f"  Test Loss: {metrics['loss']:.4f}")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  F1 (macro): {metrics['f1_macro']:.4f}")
        print(f"  F1 (weighted): {metrics['f1_weighted']:.4f}")
        print(f"  Precision (macro): {metrics['precision_macro']:.4f}")
        print(f"  Recall (macro): {metrics['recall_macro']:.4f}")
        if metrics['roc_auc_ovr'] is not None:
            print(f"  ROC-AUC (OvR macro): {metrics['roc_auc_ovr']:.4f}")

        print(f"\n  í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print("-" * 60)
        report = classification_report(
            labels, preds,
            labels=list(range(len(self.class_names))),
            target_names=self.class_names,
            zero_division=0
        )
        print(report)

        print(f"\n  í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
        for i, (name, count) in enumerate(zip(self.class_names, metrics['class_counts'])):
            print(f"    {i}: {name}: {count}")

        print(f"{'='*60}\n")

    def _log_to_tensorboard(self, metrics: dict, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray):
        """TensorBoardì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹… (5í´ë˜ìŠ¤ ë‹¤ì¤‘ë¶„ë¥˜)"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        print("ğŸ“Š TensorBoard ë¡œê¹… ì¤‘...")

        # 1. Scalars - ê¸°ë³¸ ì§€í‘œ
        self.writer.add_scalar('Test/Loss', metrics['loss'], 0)
        self.writer.add_scalar('Test/Accuracy', metrics['accuracy'], 0)
        self.writer.add_scalar('Test/F1_macro', metrics['f1_macro'], 0)
        self.writer.add_scalar('Test/F1_weighted', metrics['f1_weighted'], 0)
        self.writer.add_scalar('Test/Precision_macro', metrics['precision_macro'], 0)
        self.writer.add_scalar('Test/Recall_macro', metrics['recall_macro'], 0)
        if metrics['roc_auc_ovr'] is not None:
            self.writer.add_scalar('Test/ROC_AUC_OvR', metrics['roc_auc_ovr'], 0)

        # 2. í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        for i, class_name in enumerate(self.class_names):
            self.writer.add_scalar(f'Test/PerClass/F1/{class_name}', metrics['f1_per_class'][i], 0)
            self.writer.add_scalar(f'Test/PerClass/Precision/{class_name}', metrics['precision_per_class'][i], 0)
            self.writer.add_scalar(f'Test/PerClass/Recall/{class_name}', metrics['recall_per_class'][i], 0)

        # 3. PR Curve (í´ë˜ìŠ¤ë³„ One-vs-Rest)
        for i, class_name in enumerate(self.class_names):
            binary_labels = (labels == i).astype(int)
            class_probs = probs[:, i]
            self.writer.add_pr_curve(
                f'Test/PR_Curve/{class_name}',
                binary_labels,
                class_probs,
                global_step=0
            )

        # 4. Confusion Matrix (5x5)
        cm = np.array(metrics['confusion_matrix'])
        fig, ax = plt.subplots(figsize=(12, 10))
        im = ax.imshow(cm, cmap='Blues', interpolation='nearest')
        ax.figure.colorbar(im, ax=ax)

        ax.set(
            xticks=np.arange(cm.shape[1]),
            yticks=np.arange(cm.shape[0]),
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ylabel='True Label',
            xlabel='Predicted Label',
            title='Confusion Matrix (Test Set) - 5 Classes'
        )
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

        # ìˆ«ì í‘œì‹œ
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black")

        fig.tight_layout()

        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', dpi=150)
        buf.seek(0)
        cm_image = Image.open(buf)
        cm_array = np.array(cm_image)
        self.writer.add_image('Test/Confusion_Matrix', cm_array, 0, dataformats='HWC')
        plt.close(fig)

        # 5. í´ë˜ìŠ¤ë³„ TP/FP/FN í…Œì´ë¸”
        self._log_error_summary_table(cm)

        # 6. í´ë˜ìŠ¤ë³„ í™•ë¥  íˆìŠ¤í† ê·¸ë¨
        for i, class_name in enumerate(self.class_names):
            class_probs = probs[:, i]

            # ì „ì²´ ìƒ˜í”Œì— ëŒ€í•œ í•´ë‹¹ í´ë˜ìŠ¤ í™•ë¥ 
            self.writer.add_histogram(f'Test/Probabilities/{class_name}/all', class_probs, 0)

            # ì‹¤ì œ í•´ë‹¹ í´ë˜ìŠ¤ì¸ ìƒ˜í”Œì˜ í™•ë¥ 
            true_mask = labels == i
            if true_mask.sum() > 0:
                self.writer.add_histogram(f'Test/Probabilities/{class_name}/true_samples', class_probs[true_mask], 0)

            # ì‹¤ì œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì¸ ìƒ˜í”Œì˜ í™•ë¥ 
            false_mask = labels != i
            if false_mask.sum() > 0:
                self.writer.add_histogram(f'Test/Probabilities/{class_name}/false_samples', class_probs[false_mask], 0)

        # 7. ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬ (ì •ë‹µ/ì˜¤ë‹µ)
        max_probs = probs.max(axis=1)
        correct_mask = preds == labels

        if correct_mask.sum() > 0:
            self.writer.add_histogram('Test/Confidence/correct', max_probs[correct_mask], 0)
        if (~correct_mask).sum() > 0:
            self.writer.add_histogram('Test/Confidence/incorrect', max_probs[~correct_mask], 0)
        self.writer.add_histogram('Test/Confidence/all', max_probs, 0)

        # 8. í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”
        self._log_class_distribution(labels)

        self.writer.flush()
        print("âœ… TensorBoard ë¡œê¹… ì™„ë£Œ")

    def _log_error_summary_table(self, cm: np.ndarray):
        """FP/FN ìš”ì•½ í…Œì´ë¸” ì´ë¯¸ì§€ë¡œ ë¡œê¹…"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        try:
            data = []
            for i, class_name in enumerate(self.class_names):
                tp = cm[i, i]
                fn = cm[i, :].sum() - tp
                fp = cm[:, i].sum() - tp
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                data.append([class_name[:20], int(tp), int(fp), int(fn), f'{precision:.3f}', f'{recall:.3f}', f'{f1:.3f}'])

            fig, ax = plt.subplots(figsize=(14, 5))
            ax.axis('off')

            table = ax.table(
                cellText=data,
                colLabels=['Class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1'],
                cellLoc='center',
                loc='center',
                colWidths=[0.28, 0.1, 0.1, 0.1, 0.12, 0.12, 0.12]
            )
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 1.8)

            for j in range(7):
                table[(0, j)].set_facecolor('#4472C4')
                table[(0, j)].set_text_props(color='white', weight='bold')

            plt.title('Test Set - Classification Errors by Class', fontsize=12, fontweight='bold', pad=20)
            plt.tight_layout()

            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            buf.seek(0)
            image = Image.open(buf)
            image_array = np.array(image)

            self.writer.add_image('Test/Error_Summary_Table', image_array, 0, dataformats='HWC')
            plt.close(fig)

        except Exception as e:
            print(f"âš ï¸ Error Summary Table ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _log_class_distribution(self, labels: np.ndarray):
        """í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”"""
        import matplotlib.pyplot as plt
        import io
        from PIL import Image

        try:
            class_counts = np.bincount(labels, minlength=self.num_classes)

            fig, ax = plt.subplots(figsize=(12, 6))
            colors = plt.cm.tab10(np.linspace(0, 1, self.num_classes))
            bars = ax.bar(range(self.num_classes), class_counts, color=colors)

            ax.set_xlabel('Class')
            ax.set_ylabel('Count')
            ax.set_title('Test Set - Class Distribution')
            ax.set_xticks(range(self.num_classes))
            ax.set_xticklabels(self.class_names, rotation=45, ha='right')

            total = class_counts.sum()
            for bar, count in zip(bars, class_counts):
                height = bar.get_height()
                ax.annotate(f'{count:,}\n({count/total*100:.1f}%)',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom', fontsize=9)

            plt.tight_layout()

            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
            buf.seek(0)
            image = Image.open(buf)
            image_array = np.array(image)

            self.writer.add_image('Test/Class_Distribution', image_array, 0, dataformats='HWC')
            plt.close(fig)

        except Exception as e:
            print(f"âš ï¸ Class Distribution ë¡œê¹… ì‹¤íŒ¨: {e}")

    def _save_results(self, metrics: dict, labels: np.ndarray, preds: np.ndarray, probs: np.ndarray):
        """ê²°ê³¼ë¥¼ JSONê³¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        checkpoint_name = Path(self.checkpoint_path).stem

        # 1. JSON íŒŒì¼ ì €ì¥ (ìƒì„¸ ê²°ê³¼)
        json_path = self.results_dir / f'test_{checkpoint_name}_{self.timestamp}.json'

        json_data = {
            'checkpoint': self.checkpoint_path,
            'timestamp': self.timestamp,
            'num_classes': self.num_classes,
            'class_names': self.class_names,
            'total_samples': metrics['total_samples'],
            'class_counts': dict(zip(self.class_names, metrics['class_counts'])),
            'metrics': {
                'loss': metrics['loss'],
                'accuracy': metrics['accuracy'],
                'f1_macro': metrics['f1_macro'],
                'f1_weighted': metrics['f1_weighted'],
                'precision_macro': metrics['precision_macro'],
                'recall_macro': metrics['recall_macro'],
                'roc_auc_ovr': metrics['roc_auc_ovr'],
            },
            'per_class_metrics': {
                name: {
                    'f1': metrics['f1_per_class'][i],
                    'precision': metrics['precision_per_class'][i],
                    'recall': metrics['recall_per_class'][i],
                    'support': metrics['class_counts'][i]
                }
                for i, name in enumerate(self.class_names)
            },
            'confusion_matrix': metrics['confusion_matrix']
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… JSON ê²°ê³¼ ì €ì¥: {json_path}")

        # 2. CSV íŒŒì¼ ì €ì¥ (ìš”ì•½)
        csv_path = self.log_dir / f'test_{checkpoint_name}_{self.timestamp}.csv'

        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # í—¤ë”
            header = ['metric', 'value']
            writer.writerow(header)

            # ì „ì²´ ë©”íŠ¸ë¦­
            writer.writerow(['checkpoint', self.checkpoint_path])
            writer.writerow(['timestamp', self.timestamp])
            writer.writerow(['total_samples', metrics['total_samples']])
            writer.writerow(['loss', f"{metrics['loss']:.4f}"])
            writer.writerow(['accuracy', f"{metrics['accuracy']:.4f}"])
            writer.writerow(['f1_macro', f"{metrics['f1_macro']:.4f}"])
            writer.writerow(['f1_weighted', f"{metrics['f1_weighted']:.4f}"])
            writer.writerow(['precision_macro', f"{metrics['precision_macro']:.4f}"])
            writer.writerow(['recall_macro', f"{metrics['recall_macro']:.4f}"])
            if metrics['roc_auc_ovr'] is not None:
                writer.writerow(['roc_auc_ovr', f"{metrics['roc_auc_ovr']:.4f}"])

            writer.writerow([])  # ë¹ˆ ì¤„
            writer.writerow(['--- Per Class Metrics ---', ''])

            # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
            for i, name in enumerate(self.class_names):
                writer.writerow([f'{name}_f1', f"{metrics['f1_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_precision', f"{metrics['precision_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_recall', f"{metrics['recall_per_class'][i]:.4f}"])
                writer.writerow([f'{name}_support', metrics['class_counts'][i]])

        print(f"âœ… CSV ê²°ê³¼ ì €ì¥: {csv_path}")

        # 3. Confusion Matrix ì´ë¯¸ì§€ ì €ì¥
        self._save_confusion_matrix_image(metrics['confusion_matrix'], checkpoint_name)

    def _save_confusion_matrix_image(self, cm: list, checkpoint_name: str):
        """Confusion Matrix ì´ë¯¸ì§€ ì €ì¥"""
        import matplotlib.pyplot as plt

        cm = np.array(cm)
        fig, ax = plt.subplots(figsize=(12, 10))
        im = ax.imshow(cm, cmap='Blues', interpolation='nearest')
        ax.figure.colorbar(im, ax=ax)

        ax.set(
            xticks=np.arange(cm.shape[1]),
            yticks=np.arange(cm.shape[0]),
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ylabel='True Label',
            xlabel='Predicted Label',
            title=f'Confusion Matrix - {checkpoint_name}'
        )
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black")

        fig.tight_layout()

        img_path = self.results_dir / f'confusion_matrix_{checkpoint_name}_{self.timestamp}.png'
        plt.savefig(img_path, dpi=150, bbox_inches='tight')
        plt.close(fig)
        print(f"âœ… Confusion Matrix ì´ë¯¸ì§€ ì €ì¥: {img_path}")

    def close(self):
        """TensorBoard Writer ë‹«ê¸°"""
        if self.writer is not None:
            self.writer.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='CT CNN Test (5-class classification)')
    parser.add_argument(
        '--checkpoint',
        type=str,
        required=True,
        help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: models/ct_cnn/checkpoints/ct_unified_best_20260105_140553.pt)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default=None,
        help='Config íŒŒì¼ ê²½ë¡œ (ìƒëµí•˜ë©´ checkpointì—ì„œ ë¡œë“œ)'
    )
    parser.add_argument(
        '--no-tensorboard',
        action='store_true',
        help='TensorBoard ë¡œê¹… ë¹„í™œì„±í™”'
    )

    args = parser.parse_args()

    # Config ë¡œë“œ (ì„ íƒì‚¬í•­)
    config = None
    if args.config:
        config = ConfigLoader.load(args.config)

    # Tester ìƒì„± ë° í‰ê°€
    tester = CNNTester(
        checkpoint_path=args.checkpoint,
        config=config,
        enable_tensorboard=not args.no_tensorboard
    )
    results = tester.test()

    # ì¶”ê°€ ë¶„ì„
    print("\nğŸ“Š ìƒì„¸ ë¶„ì„:")
    preds_data = results['predictions']
    labels = preds_data['labels']
    preds = preds_data['preds']
    probs = preds_data['probs']

    # í´ë˜ìŠ¤ë³„ ì˜¤ë‹µ ë¶„ì„
    print("\n  í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ ë¶„ì„:")
    for i, class_name in enumerate(tester.class_names):
        # í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡ (FN)
        fn_mask = (labels == i) & (preds != i)
        fn_count = fn_mask.sum()

        # ë‹¤ë¥¸ ê²ƒì„ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ì˜ëª» ì˜ˆì¸¡ (FP)
        fp_mask = (labels != i) & (preds == i)
        fp_count = fp_mask.sum()

        print(f"    {class_name}:")
        print(f"      - FN (ë†“ì¹œ ê²ƒ): {fn_count}ê°œ")
        print(f"      - FP (ì˜ëª» ì˜ˆì¸¡): {fp_count}ê°œ")

    # ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„
    max_probs = probs.max(axis=1)
    correct_mask = preds == labels

    print(f"\n  ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„:")
    print(f"    ì •ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„: {max_probs[correct_mask].mean():.4f} (std: {max_probs[correct_mask].std():.4f})")
    if (~correct_mask).sum() > 0:
        print(f"    ì˜¤ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„: {max_probs[~correct_mask].mean():.4f} (std: {max_probs[~correct_mask].std():.4f})")

    # TensorBoard Writer ë‹«ê¸°
    tester.close()

    print(f"\n{'='*60}")
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   - TensorBoard: tensorboard --logdir={tester.tb_log_dir}")
    print(f"   - ê²°ê³¼ íŒŒì¼: {tester.results_dir}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
