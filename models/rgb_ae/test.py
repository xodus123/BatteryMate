"""RGB AutoEncoder í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (TensorBoard ë¡œê¹… í¬í•¨)"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
_project_root = Path(__file__).parent.parent.parent.absolute()
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
import numpy as np
from tqdm import tqdm
import argparse
from datetime import datetime
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score

from models.rgb_ae.model import create_model, ConvAutoEncoder
from training.configs.config_loader import ConfigLoader
from training.data.dataset import BatteryDataset
from training.data.transforms import get_transforms, get_albumentations_transforms


class AETester:
    """AutoEncoder í…ŒìŠ¤í„°"""

    def __init__(self, checkpoint_path: str, config: dict = None, log_dir: str = None):
        """
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            config: YAML config dict (Noneì´ë©´ checkpointì—ì„œ ë¡œë“œ)
            log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.checkpoint_path = checkpoint_path

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

        # Config ë¡œë“œ
        self.config = checkpoint.get('config', config)
        if self.config is None:
            raise ValueError("Configë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model = create_model(self.config).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()

        # Threshold ë¡œë“œ
        self.threshold = checkpoint.get('threshold', None)
        if self.threshold is None:
            # threshold.jsonì—ì„œ ë¡œë“œ ì‹œë„
            threshold_path = Path(self.config['checkpoint']['save_dir']) / 'threshold.json'
            if threshold_path.exists():
                with open(threshold_path, 'r') as f:
                    threshold_data = json.load(f)
                    self.threshold = threshold_data.get('threshold', 0.1)
            else:
                self.threshold = 0.1  # ê¸°ë³¸ê°’

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        self.test_loader = self._create_test_dataloader()

        # TensorBoard Writer
        if log_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_dir = f"models/rgb_ae/logs/test_{timestamp}"
        self.log_dir = log_dir
        self.writer = SummaryWriter(log_dir=log_dir)

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"   - Epoch: {checkpoint.get('epoch', 'N/A')}")
        print(f"   - Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}")
        print(f"   - Threshold: {self.threshold:.4f}")
        print(f"   - Device: {self.device}")
        print(f"   - Test ë°ì´í„°: {len(self.test_loader.dataset)}ê°œ")
        print(f"   - TensorBoard: {log_dir}\n")

    def _create_test_dataloader(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„±"""
        data_config = self.config['data']
        image_size = data_config['image_size']
        preprocessed = data_config.get('preprocessed', False)
        use_albumentations = data_config.get('use_albumentations', False)

        # Transform ì„ íƒ
        if use_albumentations:
            transform = get_albumentations_transforms('rgb', 'test', image_size, preprocessed)
        else:
            transform = get_transforms('rgb', 'test', image_size, preprocessed)

        test_dataset = BatteryDataset(
            split_file=data_config['test_split'],
            transform=transform,
            modality='rgb',
            preprocessed=preprocessed
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=data_config['batch_size'],
            shuffle=False,
            num_workers=data_config.get('num_workers', 8),
            pin_memory=True
        )

        return test_loader

    @torch.no_grad()
    def test(self) -> dict:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€"""
        print(f"{'='*60}")
        print(f"Test ë°ì´í„° í‰ê°€ ì‹œì‘")
        print(f"{'='*60}\n")

        all_scores = []
        all_labels = []
        all_preds = []

        for images, labels in tqdm(self.test_loader, desc="Testing"):
            images = images.to(self.device)

            # ì´ìƒ ì ìˆ˜ ê³„ì‚°
            scores = self.model.get_anomaly_score(images)

            all_scores.extend(scores.cpu().numpy())
            all_labels.extend(labels.numpy())

        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)

        # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (0: normal, 1+: defect)
        binary_labels = (all_labels > 0).astype(int)

        # Threshold ê¸°ë°˜ ì˜ˆì¸¡
        # ì‹¤ì œ ê²°ê³¼: Defect(ë¶ˆëŸ‰)ì´ ë†’ì€ ì ìˆ˜, Normal(ì •ìƒ)ì´ ë‚®ì€ ì ìˆ˜
        # (ë¶ˆëŸ‰ ë°ì´í„°ì˜ ë³€ë™ì„±ì´ ì»¤ì„œ ëª¨ë¸ì´ í‰ê·  íŒ¨í„´ í•™ìŠµ â†’ ì •ìƒì´ ë” ì˜ ì¬êµ¬ì„±ë¨)
        # ë”°ë¼ì„œ: ì ìˆ˜ > threshold â†’ defect(1), ì ìˆ˜ <= threshold â†’ normal(0)
        all_preds = (all_scores > self.threshold).astype(int)  # ë†’ì€ ì ìˆ˜ = defect

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_metrics(all_scores, all_labels, binary_labels, all_preds)

        # ê²°ê³¼ ì¶œë ¥
        self._print_results(metrics)

        # TensorBoard ë¡œê¹…
        self._log_to_tensorboard(metrics, all_scores, all_labels, binary_labels, all_preds)

        return {
            'metrics': metrics,
            'scores': all_scores,
            'labels': all_labels,
            'predictions': all_preds
        }

    def _calculate_metrics(self, scores, labels, binary_labels, preds) -> dict:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}

        # ê¸°ë³¸ í†µê³„
        metrics['num_samples'] = len(labels)
        metrics['num_normal'] = (labels == 0).sum()
        metrics['num_defect'] = (labels > 0).sum()

        # ì ìˆ˜ í†µê³„
        normal_scores = scores[labels == 0]
        defect_scores = scores[labels > 0]

        metrics['score_mean'] = scores.mean()
        metrics['score_std'] = scores.std()

        if len(normal_scores) > 0:
            metrics['normal_score_mean'] = normal_scores.mean()
            metrics['normal_score_std'] = normal_scores.std()
        if len(defect_scores) > 0:
            metrics['defect_score_mean'] = defect_scores.mean()
            metrics['defect_score_std'] = defect_scores.std()

        # ROC-AUC (defectë¥¼ positiveë¡œ, ë†’ì€ ì ìˆ˜ = defect)
        try:
            metrics['roc_auc'] = roc_auc_score(binary_labels, scores)
        except:
            metrics['roc_auc'] = 0.0

        # Accuracy, F1 (threshold ê¸°ë°˜)
        metrics['accuracy'] = accuracy_score(binary_labels, preds)
        metrics['f1'] = f1_score(binary_labels, preds, zero_division=0)

        # Confusion Matrix
        cm = confusion_matrix(binary_labels, preds)
        metrics['confusion_matrix'] = cm.tolist()

        # ìµœì  threshold ì°¾ê¸°
        fpr, tpr, thresholds = roc_curve(binary_labels, scores)
        optimal_idx = np.argmax(tpr - fpr)
        metrics['optimal_threshold'] = thresholds[optimal_idx]

        return metrics

    def _print_results(self, metrics: dict):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"Test ê²°ê³¼")
        print(f"{'='*60}")
        print(f"  ìƒ˜í”Œ ìˆ˜: {metrics['num_samples']}")
        print(f"    - Normal: {metrics['num_normal']}")
        print(f"    - Defect: {metrics['num_defect']}")
        print()
        print(f"  ì ìˆ˜ í†µê³„:")
        if 'normal_score_mean' in metrics:
            print(f"    - Normal: {metrics['normal_score_mean']:.4f} Â± {metrics['normal_score_std']:.4f}")
        if 'defect_score_mean' in metrics:
            print(f"    - Defect: {metrics['defect_score_mean']:.4f} Â± {metrics['defect_score_std']:.4f}")
        print()
        print(f"  ì„±ëŠ¥ ì§€í‘œ:")
        print(f"    - ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"    - Accuracy: {metrics['accuracy']:.4f}")
        print(f"    - F1 Score: {metrics['f1']:.4f}")
        print(f"    - Threshold: {self.threshold:.4f}")
        print(f"    - Optimal Threshold: {metrics['optimal_threshold']:.4f}")
        print(f"{'='*60}\n")

    def _log_to_tensorboard(self, metrics: dict, scores: np.ndarray, labels: np.ndarray,
                            binary_labels: np.ndarray, preds: np.ndarray):
        """TensorBoardì— ê²°ê³¼ ë¡œê¹…"""
        # 1. ìŠ¤ì¹¼ë¼ ë©”íŠ¸ë¦­ ë¡œê¹…
        self.writer.add_scalar('Test/ROC_AUC', metrics['roc_auc'], 0)
        self.writer.add_scalar('Test/Accuracy', metrics['accuracy'], 0)
        self.writer.add_scalar('Test/F1_Score', metrics['f1'], 0)
        self.writer.add_scalar('Test/Threshold', self.threshold, 0)
        self.writer.add_scalar('Test/Optimal_Threshold', metrics['optimal_threshold'], 0)

        if 'normal_score_mean' in metrics:
            self.writer.add_scalar('Test/Normal_Score_Mean', metrics['normal_score_mean'], 0)
            self.writer.add_scalar('Test/Normal_Score_Std', metrics['normal_score_std'], 0)
        if 'defect_score_mean' in metrics:
            self.writer.add_scalar('Test/Defect_Score_Mean', metrics['defect_score_mean'], 0)
            self.writer.add_scalar('Test/Defect_Score_Std', metrics['defect_score_std'], 0)

        # 2. Score Distribution íˆìŠ¤í† ê·¸ë¨
        normal_scores = scores[labels == 0]
        defect_scores = scores[labels > 0]

        if len(normal_scores) > 0:
            self.writer.add_histogram('Test/Normal_Scores', normal_scores, 0)
        if len(defect_scores) > 0:
            self.writer.add_histogram('Test/Defect_Scores', defect_scores, 0)

        # 3. Confusion Matrix Figure
        cm = confusion_matrix(binary_labels, preds)
        fig_cm = plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Normal', 'Defect'],
                    yticklabels=['Normal', 'Defect'])
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title(f'Confusion Matrix (Threshold={self.threshold:.4f})')
        self.writer.add_figure('Test/Confusion_Matrix', fig_cm, 0)
        plt.close(fig_cm)

        # 4. ROC Curve Figure
        fpr, tpr, _ = roc_curve(binary_labels, scores)
        fig_roc = plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC={metrics["roc_auc"]:.4f})')
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend(loc='lower right')
        plt.grid(True, alpha=0.3)
        self.writer.add_figure('Test/ROC_Curve', fig_roc, 0)
        plt.close(fig_roc)

        # 5. Score Distribution Figure
        fig_dist = plt.figure(figsize=(10, 5))
        plt.hist(normal_scores, bins=50, alpha=0.7, label=f'Normal (n={len(normal_scores)})', color='green')
        plt.hist(defect_scores, bins=50, alpha=0.7, label=f'Defect (n={len(defect_scores)})', color='red')
        plt.axvline(self.threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={self.threshold:.4f}')
        plt.xlabel('Anomaly Score')
        plt.ylabel('Count')
        plt.title('Score Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        self.writer.add_figure('Test/Score_Distribution', fig_dist, 0)
        plt.close(fig_dist)

        # 6. Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(binary_labels, scores)
        fig_pr = plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, 'b-', linewidth=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.grid(True, alpha=0.3)
        self.writer.add_figure('Test/PR_Curve', fig_pr, 0)
        plt.close(fig_pr)

        # 7. ìƒ˜í”Œ ì¬êµ¬ì„± ê²°ê³¼ ì´ë¯¸ì§€ ë¡œê¹…
        self._log_reconstructions_to_tensorboard()

        self.writer.flush()
        print(f"âœ… TensorBoard ë¡œê¹… ì™„ë£Œ: {self.log_dir}")

    def _log_reconstructions_to_tensorboard(self, num_samples: int = 8):
        """ì¬êµ¬ì„± ê²°ê³¼ë¥¼ TensorBoardì— ë¡œê¹…"""
        dataiter = iter(self.test_loader)
        images, labels = next(dataiter)
        images = images[:num_samples].to(self.device)

        with torch.no_grad():
            reconstructed, _ = self.model(images)

        # Denormalize
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(self.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(self.device)

        images_denorm = torch.clamp(images * std + mean, 0, 1)
        reconstructed_denorm = torch.clamp(reconstructed * std + mean, 0, 1)

        # ì›ë³¸, ì¬êµ¬ì„±, ì°¨ì´ë¥¼ ê²°í•©
        diff = torch.abs(images_denorm - reconstructed_denorm)

        # Grid ì´ë¯¸ì§€ ìƒì„±
        import torchvision.utils as vutils

        # ì›ë³¸ ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ
        self.writer.add_images('Test/Original', images_denorm.cpu(), 0)
        # ì¬êµ¬ì„± ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ
        self.writer.add_images('Test/Reconstructed', reconstructed_denorm.cpu(), 0)
        # ì°¨ì´ ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ
        self.writer.add_images('Test/Difference', diff.cpu(), 0)

    def visualize_reconstructions(self, num_samples: int = 8, save_path: str = None):
        """ì¬êµ¬ì„± ê²°ê³¼ ì‹œê°í™”"""
        self.model.eval()

        # ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        dataiter = iter(self.test_loader)
        images, labels = next(dataiter)
        images = images[:num_samples].to(self.device)
        labels = labels[:num_samples]

        with torch.no_grad():
            reconstructed, _ = self.model(images)

        # Denormalize
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(self.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(self.device)

        images_denorm = images * std + mean
        reconstructed_denorm = reconstructed * std + mean

        # í´ë¨í•‘
        images_denorm = torch.clamp(images_denorm, 0, 1)
        reconstructed_denorm = torch.clamp(reconstructed_denorm, 0, 1)

        # í”Œë¡¯
        fig, axes = plt.subplots(3, num_samples, figsize=(2*num_samples, 6))

        class_names = ['normal', 'pollution', 'mixed']

        for i in range(num_samples):
            # ì›ë³¸
            axes[0, i].imshow(images_denorm[i].cpu().permute(1, 2, 0).numpy())
            axes[0, i].set_title(f'{class_names[labels[i]]}')
            axes[0, i].axis('off')

            # ì¬êµ¬ì„±
            axes[1, i].imshow(reconstructed_denorm[i].cpu().permute(1, 2, 0).numpy())
            axes[1, i].set_title('Reconstructed')
            axes[1, i].axis('off')

            # ì°¨ì´
            diff = torch.abs(images_denorm[i] - reconstructed_denorm[i]).mean(dim=0)
            axes[2, i].imshow(diff.cpu().numpy(), cmap='hot')
            axes[2, i].set_title(f'Error: {diff.mean():.3f}')
            axes[2, i].axis('off')

        axes[0, 0].set_ylabel('Original', fontsize=12)
        axes[1, 0].set_ylabel('Reconstructed', fontsize=12)
        axes[2, 0].set_ylabel('Error Map', fontsize=12)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"âœ… ì‹œê°í™” ì €ì¥: {save_path}")
        else:
            plt.show()

        plt.close()

    def plot_score_distribution(self, scores: np.ndarray, labels: np.ndarray, save_path: str = None):
        """ì´ìƒ ì ìˆ˜ ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # íˆìŠ¤í† ê·¸ë¨
        normal_scores = scores[labels == 0]
        defect_scores = scores[labels > 0]

        axes[0].hist(normal_scores, bins=50, alpha=0.7, label=f'Normal (n={len(normal_scores)})', color='green')
        axes[0].hist(defect_scores, bins=50, alpha=0.7, label=f'Defect (n={len(defect_scores)})', color='red')
        axes[0].axvline(self.threshold, color='black', linestyle='--', label=f'Threshold={self.threshold:.3f}')
        axes[0].set_xlabel('Anomaly Score')
        axes[0].set_ylabel('Count')
        axes[0].set_title('Score Distribution')
        axes[0].legend()

        # ROC Curve (defect = positive, ë†’ì€ ì ìˆ˜ = defect)
        binary_labels = (labels > 0).astype(int)
        fpr, tpr, _ = roc_curve(binary_labels, scores)
        auc = roc_auc_score(binary_labels, scores)

        axes[1].plot(fpr, tpr, label=f'ROC (AUC={auc:.3f})')
        axes[1].plot([0, 1], [0, 1], 'k--')
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        axes[1].set_title('ROC Curve')
        axes[1].legend()

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"âœ… ë¶„í¬ ì‹œê°í™” ì €ì¥: {save_path}")
        else:
            plt.show()

        plt.close()


def main():
    parser = argparse.ArgumentParser(description='RGB AutoEncoder Testing')
    parser.add_argument('--checkpoint', type=str, required=True, help='ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--config', type=str, default='autoencoder_rgb', help='Config íŒŒì¼ ì´ë¦„')
    parser.add_argument('--visualize', action='store_true', help='ì¬êµ¬ì„± ê²°ê³¼ ì‹œê°í™”')
    parser.add_argument('--save-dir', type=str, default='models/rgb_ae/results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--log-dir', type=str, default=None, help='TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬')
    args = parser.parse_args()

    # TensorBoard ì‹¤í–‰ ì•ˆë‚´
    print(f"\n{'='*60}")
    print(f"ğŸ“Š TensorBoard ì‹¤í–‰ ëª…ë ¹ì–´:")
    print(f"   tensorboard --logdir=models/rgb_ae/logs --port=6007")
    print(f"   http://localhost:6007")
    print(f"{'='*60}\n")

    # Config ë¡œë“œ (ì„ íƒì )
    config = None
    if args.config:
        try:
            config_loader = ConfigLoader()
            config = config_loader.load(args.config)
        except:
            pass

    # Tester ìƒì„± ë° í…ŒìŠ¤íŠ¸
    tester = AETester(checkpoint_path=args.checkpoint, config=config, log_dir=args.log_dir)
    results = tester.test()

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # ì‹œê°í™”
    if args.visualize:
        tester.visualize_reconstructions(
            num_samples=8,
            save_path=save_dir / 'reconstructions.png'
        )
        tester.plot_score_distribution(
            results['scores'],
            results['labels'],
            save_path=save_dir / 'score_distribution.png'
        )

    # ê²°ê³¼ JSON ì €ì¥
    def convert_to_serializable(obj):
        if isinstance(obj, (np.floating, float)):
            return float(obj)
        elif isinstance(obj, (np.integer, int)):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj

    results_json = {
        'checkpoint': args.checkpoint,
        'metrics': {k: convert_to_serializable(v)
                   for k, v in results['metrics'].items()
                   if k != 'confusion_matrix'},
        'confusion_matrix': results['metrics']['confusion_matrix'],
        'threshold': float(tester.threshold),
        'timestamp': datetime.now().isoformat()
    }

    with open(save_dir / 'test_results.json', 'w') as f:
        json.dump(results_json, f, indent=2)
    print(f"âœ… ê²°ê³¼ ì €ì¥: {save_dir / 'test_results.json'}")

    # TensorBoard writer ì¢…ë£Œ
    tester.writer.close()
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   TensorBoard í™•ì¸: tensorboard --logdir=models/rgb_ae/logs --port=6007")


if __name__ == "__main__":
    main()
