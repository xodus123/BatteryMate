"""VLM í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def test_prompts():
    """í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸"""
    # prompts ëª¨ë“ˆë§Œ ì§ì ‘ import (transformers ì˜ì¡´ì„± ì—†ìŒ)
    sys.path.insert(0, str(Path(__file__).parent))
    from prompts import BatteryDefectPrompts

    print("=" * 60)
    print("VLM í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # CT í”„ë¡¬í”„íŠ¸
    print("\n[CT ìƒì„¸ í”„ë¡¬í”„íŠ¸]")
    print(BatteryDefectPrompts.get_ct_prompt(detailed=True)[:200] + "...")

    print("\n[CT ê°„ë‹¨ í”„ë¡¬í”„íŠ¸]")
    print(BatteryDefectPrompts.get_ct_prompt(detailed=False))

    # RGB í”„ë¡¬í”„íŠ¸
    print("\n[RGB ìƒì„¸ í”„ë¡¬í”„íŠ¸]")
    print(BatteryDefectPrompts.get_rgb_prompt(detailed=True)[:200] + "...")

    # Zero-shot í”„ë¡¬í”„íŠ¸
    print("\n[Zero-shot í”„ë¡¬í”„íŠ¸]")
    print(BatteryDefectPrompts.ZERO_SHOT_CLASSIFICATION[:200] + "...")

    print("\nâœ… í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def test_inference_mock():
    """ì¶”ë¡  ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (ëª¨ë¸ ë¡œë“œ ì—†ì´)"""
    print("\n" + "=" * 60)
    print("VLM ì¶”ë¡  ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (Mock)")
    print("=" * 60)

    try:
        from models.vlm.inference import VLMInference

        # ëª¨ë¸ í¬ê¸° í™•ì¸
        print("\n[ì§€ì› ëª¨ë¸ í¬ê¸°]")
        for size, name in VLMInference.MODEL_SIZES.items():
            print(f"  - {size}: {name}")

        # ê²°í•¨ í´ë˜ìŠ¤ í™•ì¸
        print("\n[ê²°í•¨ í´ë˜ìŠ¤]")
        for modality, classes in VLMInference.DEFECT_CLASSES.items():
            print(f"  - {modality}: {classes}")

        print("\nâœ… ì¶”ë¡  ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    except ImportError as e:
        print(f"\nâš ï¸ ì˜ì¡´ì„± ë¯¸ì„¤ì¹˜: {e}")
        print("   ì„¤ì¹˜: pip install transformers qwen-vl-utils")

        # ëª¨ë¸ ì •ë³´ë§Œ ì¶œë ¥
        print("\n[ì§€ì› ëª¨ë¸ í¬ê¸°]")
        model_sizes = {
            '2b': 'Qwen/Qwen2-VL-2B-Instruct',
            '7b': 'Qwen/Qwen2-VL-7B-Instruct',
            '72b': 'Qwen/Qwen2-VL-72B-Instruct',
        }
        for size, name in model_sizes.items():
            print(f"  - {size}: {name}")


def test_inference_with_model():
    """ì‹¤ì œ ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    try:
        from models.vlm.inference import create_vlm_inference
        from PIL import Image
        import torch

        print("\n" + "=" * 60)
        print("VLM ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        # GPU í™•ì¸
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

        # ëª¨ë¸ ë¡œë“œ (2B ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸)
        print("\nëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        vlm = create_vlm_inference(model_size='2b', device=device)

        # ëª¨ë¸ ì •ë³´
        info = vlm.get_model_info()
        print(f"\n[ëª¨ë¸ ì •ë³´]")
        for key, value in info.items():
            print(f"  - {key}: {value}")

        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì°¾ê¸°
        test_images = list(PROJECT_ROOT.glob("data/**/images/*.jpg"))[:3]

        if test_images:
            print(f"\ní…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ {len(test_images)}ê°œ ë°œê²¬")

            for img_path in test_images:
                print(f"\në¶„ì„ ì¤‘: {img_path.name}")
                result = vlm.analyze_image(str(img_path), modality='ct')

                print(f"  ì˜ˆì¸¡: {result.get('prediction')}")
                print(f"  ì •ìƒ ì—¬ë¶€: {result.get('is_normal')}")
                print(f"  ì‹ ë¢°ë„: {result.get('confidence')}")
                if result.get('defect_type'):
                    print(f"  ê²°í•¨ ìœ í˜•: {result.get('defect_type')}")
        else:
            print("\ní…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            print("ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸...")
            dummy_image = Image.new('RGB', (512, 512), color='gray')
            result = vlm.analyze_image(dummy_image, modality='ct')
            print(f"  ì˜ˆì¸¡: {result.get('prediction')}")

        print("\nâœ… ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâš ï¸ ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("(ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description='VLM í…ŒìŠ¤íŠ¸')
    parser.add_argument('--full', action='store_true', help='ëª¨ë¸ ë¡œë“œ í¬í•¨ ì „ì²´ í…ŒìŠ¤íŠ¸')
    args = parser.parse_args()

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    test_prompts()
    test_inference_mock()

    # ì „ì²´ í…ŒìŠ¤íŠ¸ (ëª¨ë¸ ë¡œë“œ í¬í•¨)
    if args.full:
        test_inference_with_model()
    else:
        print("\nğŸ’¡ ì „ì²´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ --full ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        print("   python models/vlm/test_vlm.py --full")


if __name__ == "__main__":
    main()
