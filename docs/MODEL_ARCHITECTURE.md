# ResNet18 ëª¨ë¸ êµ¬ì¡°

> **ì‘ì„±ì¼**: 2024-12-29
> **ëª¨ë¸**: ResNet18 (ë°°í„°ë¦¬ ë¶ˆëŸ‰ ê²€ì‚¬ìš©)
> **ì…ë ¥ í¬ê¸°**: 512x512x3
> **ì¶œë ¥**: 2 classes (ì •ìƒ/ë¶ˆëŸ‰)

---

## ğŸ“Š ì „ì²´ íë¦„ë„

```
ì…ë ¥ ì´ë¯¸ì§€ (3, 512, 512)
    â†“
[Conv1] 7x7 conv, 64 filters, stride=2
    â†’ (64, 256, 256)
    â†“
[BatchNorm + ReLU + MaxPool]
    â†’ (64, 128, 128)
    â†“
[Layer1] 2ê°œ BasicBlock (64 channels)
    â†’ (64, 128, 128)
    â†“
[Layer2] 2ê°œ BasicBlock (128 channels, stride=2)
    â†’ (128, 64, 64)
    â†“
[Layer3] 2ê°œ BasicBlock (256 channels, stride=2)
    â†’ (256, 32, 32)
    â†“
[Layer4] 2ê°œ BasicBlock (512 channels, stride=2)
    â†’ (512, 16, 16)
    â†“
[Global Average Pooling]
    â†’ (512, 1, 1)
    â†“
[Flatten]
    â†’ (512)
    â†“
[Fully Connected] 512 â†’ 2
    â†’ (2)  [ì •ìƒ, ë¶ˆëŸ‰]
```

---

## ğŸ—ï¸ ë ˆì´ì–´ë³„ ìƒì„¸ êµ¬ì¡°

### 1. ì´ˆê¸° ë ˆì´ì–´

| ë ˆì´ì–´ | íŒŒë¼ë¯¸í„° | ì…ë ¥ í¬ê¸° | ì¶œë ¥ í¬ê¸° |
|--------|---------|-----------|-----------|
| **Conv1** | kernel=7x7, filters=64, stride=2, padding=3 | (3, 512, 512) | (64, 256, 256) |
| **BatchNorm1** | - | (64, 256, 256) | (64, 256, 256) |
| **ReLU** | - | (64, 256, 256) | (64, 256, 256) |
| **MaxPool** | kernel=3x3, stride=2, padding=1 | (64, 256, 256) | (64, 128, 128) |

### 2. Residual Layers

#### Layer1 (64 channels)
- **BasicBlock x 2**
- ì…ë ¥: (64, 128, 128)
- ì¶œë ¥: (64, 128, 128)
- íŒŒë¼ë¯¸í„°: ~147K

#### Layer2 (128 channels)
- **BasicBlock x 2**
- ì…ë ¥: (64, 128, 128)
- ì¶œë ¥: (128, 64, 64)
- Stride=2 (ì²« ë²ˆì§¸ ë¸”ë¡)
- íŒŒë¼ë¯¸í„°: ~525K

#### Layer3 (256 channels)
- **BasicBlock x 2**
- ì…ë ¥: (128, 64, 64)
- ì¶œë ¥: (256, 32, 32)
- Stride=2 (ì²« ë²ˆì§¸ ë¸”ë¡)
- íŒŒë¼ë¯¸í„°: ~2.1M

#### Layer4 (512 channels)
- **BasicBlock x 2**
- ì…ë ¥: (256, 32, 32)
- ì¶œë ¥: (512, 16, 16)
- Stride=2 (ì²« ë²ˆì§¸ ë¸”ë¡)
- íŒŒë¼ë¯¸í„°: ~8.4M

### 3. ì¶œë ¥ ë ˆì´ì–´

| ë ˆì´ì–´ | íŒŒë¼ë¯¸í„° | ì…ë ¥ í¬ê¸° | ì¶œë ¥ í¬ê¸° |
|--------|---------|-----------|-----------|
| **AdaptiveAvgPool** | output_size=(1, 1) | (512, 16, 16) | (512, 1, 1) |
| **Flatten** | - | (512, 1, 1) | (512) |
| **FC (Fully Connected)** | in=512, out=2 | (512) | (2) |

---

## ğŸ” BasicBlock êµ¬ì¡°

ResNet18ì˜ ê¸°ë³¸ ë¹Œë”© ë¸”ë¡:

```python
class BasicBlock:
    def forward(x):
        identity = x  # ì…ë ¥ ì €ì¥ (Residual Connection)

        # ì²« ë²ˆì§¸ Conv Block
        out = Conv2d(x)         # 3x3 convolution
        out = BatchNorm2d(out)
        out = ReLU(out)

        # ë‘ ë²ˆì§¸ Conv Block
        out = Conv2d(out)       # 3x3 convolution
        out = BatchNorm2d(out)

        # Residual Connection
        out = out + identity    # Skip connection
        out = ReLU(out)

        return out
```

**í•µì‹¬ ì›ë¦¬:**
- ì…ë ¥ì„ ì¶œë ¥ì— ì§ì ‘ ë”í•¨ (Shortcut/Skip Connection)
- ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing) ë¬¸ì œ í•´ê²°
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥

---

## ğŸ“ˆ ëª¨ë¸ í†µê³„

### ì „ì²´ íŒŒë¼ë¯¸í„°

| í•­ëª© | ê°’ |
|------|-----|
| **ì´ ë ˆì´ì–´ ìˆ˜** | 18ê°œ (Convolutional ë ˆì´ì–´ ê¸°ì¤€) |
| **ì´ íŒŒë¼ë¯¸í„°** | 11,177,538ê°œ (ì•½ 1,120ë§Œê°œ) |
| **í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°** | 11,177,538ê°œ (ì „ì²´) |
| **ëª¨ë¸ í¬ê¸° (float32)** | ~43 MB |
| **ëª¨ë¸ í¬ê¸° (ì²´í¬í¬ì¸íŠ¸)** | ~129 MB (optimizer state í¬í•¨) |

### ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ë¶„í¬

```
Conv1 + BN1:        ~10K   (0.1%)
Layer1:            ~147K   (1.3%)
Layer2:            ~525K   (4.7%)
Layer3:           ~2.1M    (18.8%)
Layer4:           ~8.4M    (75.2%)
FC:                 ~1K    (0.01%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:           ~11.2M   (100%)
```

**íŠ¹ì§•:**
- Layer4ê°€ ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 75%ë¥¼ ì°¨ì§€
- ê¹Šì€ ë ˆì´ì–´ì¼ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ìŒ
- ê³ ìˆ˜ì¤€ íŠ¹ì§•(high-level features) ì¶”ì¶œì— ì§‘ì¤‘

---

## ğŸ¯ í˜„ì¬ ì„¤ì •

### í•™ìŠµ ì„¤ì • (`training/configs/cnn.yaml`)

```yaml
model:
  name: resnet18
  pretrained: true          # ImageNet-1K pretrained
  num_classes: 2            # ì •ìƒ/ë¶ˆëŸ‰

data:
  image_size: 512           # 512x512 ì…ë ¥
  batch_size: 32
  num_workers: 4

training:
  optimizer: Adam
  lr: 0.0001
  weight_decay: 0.0001
  epochs: 30

criteria:
  early_stopping:
    patience: 5
    monitor: val_loss
```

---

## âœ… ì¥ì 

### 1. ImageNet Pretrained ì‚¬ìš©
- 1,400ë§Œ ì¥ì˜ ì´ë¯¸ì§€ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜
- Transfer Learningìœ¼ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
- ì¼ë°˜ì ì¸ íŠ¹ì§•(ì—ì§€, í…ìŠ¤ì²˜ ë“±) ì´ë¯¸ í•™ìŠµë¨

### 2. ì „ì²´ ë ˆì´ì–´ í•™ìŠµ (Fine-tuning)
- ëª¨ë“  11M íŒŒë¼ë¯¸í„°ê°€ í•™ìŠµë¨
- ë°°í„°ë¦¬ CT ë°ì´í„°ì— ì™„ì „íˆ ìµœì í™” ê°€ëŠ¥
- ì €ìˆ˜ì¤€ íŠ¹ì§•ë¶€í„° ê³ ìˆ˜ì¤€ íŠ¹ì§•ê¹Œì§€ ì¡°ì •

### 3. ì ì ˆí•œ ëª¨ë¸ í¬ê¸°
- ResNet18: ë„ˆë¬´ ê¹Šì§€ë„ ì–•ì§€ë„ ì•ŠìŒ
- CT ë°ì´í„° ~5,000ì¥ì— ì í•©
- ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ

### 4. Residual Connection
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ ì•ˆì •ì  í•™ìŠµ
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°
- ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

---

## ğŸ”„ ê° ë ˆì´ì–´ì˜ ì—­í• 

### Conv1 + Layer1 (ì´ˆê¸° ë ˆì´ì–´)
**í•™ìŠµí•˜ëŠ” ê²ƒ:**
- ì—ì§€ (edge)
- ì½”ë„ˆ (corner)
- ê¸°ë³¸ í…ìŠ¤ì²˜ (texture)

**ë°°í„°ë¦¬ ê²€ì‚¬ì—ì„œ:**
- í‘œë©´ ê±°ì¹ ê¸°
- ê²½ê³„ì„ 
- ê¸°ë³¸ íŒ¨í„´

### Layer2 + Layer3 (ì¤‘ê°„ ë ˆì´ì–´)
**í•™ìŠµí•˜ëŠ” ê²ƒ:**
- ë³µí•© íŒ¨í„´
- í˜•íƒœ (shape)
- ë¶€ë¶„ì  ê°ì²´

**ë°°í„°ë¦¬ ê²€ì‚¬ì—ì„œ:**
- í¬ë™ íŒ¨í„´
- ë¶ˆê·œì¹™í•œ ì˜ì—­
- ì´ë¬¼ì§ˆ í˜•íƒœ

### Layer4 + FC (ê¹Šì€ ë ˆì´ì–´)
**í•™ìŠµí•˜ëŠ” ê²ƒ:**
- ê³ ìˆ˜ì¤€ ì˜ë¯¸ (semantic)
- ì „ì²´ì ì¸ ë§¥ë½
- í´ë˜ìŠ¤ êµ¬ë¶„ íŠ¹ì§•

**ë°°í„°ë¦¬ ê²€ì‚¬ì—ì„œ:**
- "ì´ê²Œ ì •ìƒì¸ê°€ ë¶ˆëŸ‰ì¸ê°€"
- ê²°í•¨ì˜ ì‹¬ê°ë„
- ì¢…í•©ì  íŒë‹¨

---

## ğŸ“Š ì…ë ¥ â†’ ì¶œë ¥ ë³€í™˜ ê³¼ì •

### ê³µê°„ í•´ìƒë„ ë³€í™”

```
512x512  (ì…ë ¥ ì´ë¯¸ì§€)
   â†“ Conv1 (stride=2)
256x256
   â†“ MaxPool (stride=2)
128x128
   â†“ Layer1 (stride=1)
128x128
   â†“ Layer2 (stride=2)
64x64
   â†“ Layer3 (stride=2)
32x32
   â†“ Layer4 (stride=2)
16x16
   â†“ Global AvgPool
1x1
```

### ì±„ë„ ìˆ˜ ë³€í™”

```
3      (RGB ì…ë ¥)
  â†“
64     (Conv1)
  â†“
64     (Layer1)
  â†“
128    (Layer2)
  â†“
256    (Layer3)
  â†“
512    (Layer4)
  â†“
2      (FC - ì •ìƒ/ë¶ˆëŸ‰)
```

**íŠ¹ì§•:**
- ê³µê°„ í•´ìƒë„ â†“ (512 â†’ 1)
- ì±„ë„ ìˆ˜ â†‘ (3 â†’ 512)
- "ìƒì„¸í•œ ìœ„ì¹˜ ì •ë³´" â†’ "ì¶”ìƒì ì¸ ì˜ë¯¸ ì •ë³´"

---

## ğŸ¨ TensorBoardì—ì„œ í™•ì¸í•˜ê¸°

í•™ìŠµ ì‹œì‘ í›„ TensorBoardì— ëª¨ë¸ êµ¬ì¡° ê·¸ë˜í”„ê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.

**í™•ì¸ ë°©ë²•:**
1. í•™ìŠµ ì‹œì‘: `python models/ct_cnn/train.py`
2. TensorBoard ì ‘ì†: `http://localhost:6006`
3. **GRAPHS** íƒ­ í´ë¦­
4. ì‹œê°ì ìœ¼ë¡œ ëª¨ë¸ êµ¬ì¡° í™•ì¸ ê°€ëŠ¥

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ResNet ë…¼ë¬¸
- **ì œëª©**: "Deep Residual Learning for Image Recognition"
- **ì €ì**: Kaiming He et al. (Microsoft Research)
- **ë°œí‘œ**: CVPR 2016
- **í•µì‹¬ ì•„ì´ë””ì–´**: Residual Connection (Skip Connection)

### PyTorch ê³µì‹ êµ¬í˜„
```python
import torchvision.models as models
model = models.resnet18(pretrained=True)
```

### ì½”ë“œ ìœ„ì¹˜
- ëª¨ë¸ ì •ì˜: `models/ct_cnn/model.py`
- í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸: `models/ct_cnn/train.py`
- ì„¤ì • íŒŒì¼: `training/configs/cnn.yaml`

---

## ğŸ”§ ëª¨ë¸ ìˆ˜ì • ê°€ì´ë“œ

### ResNet50ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´:

**1. Config ìˆ˜ì •** (`training/configs/cnn.yaml`):
```yaml
model:
  name: resnet50  # resnet18 â†’ resnet50
```

**2. ëª¨ë¸ íŒŒì¼ ìˆ˜ì •** (`models/ct_cnn/model.py`):
```python
self.model = models.resnet50(pretrained=pretrained)  # resnet18 â†’ resnet50
```

**ì°¨ì´ì :**
- ResNet18: 11M íŒŒë¼ë¯¸í„°, 18 ë ˆì´ì–´
- ResNet50: 25M íŒŒë¼ë¯¸í„°, 50 ë ˆì´ì–´
- ResNet50ì´ ë” ê°•ë ¥í•˜ì§€ë§Œ í•™ìŠµ ì‹œê°„ ì¦ê°€

---

## ğŸš€ ì„±ëŠ¥ ê°œì„  ì „ëµ

### ğŸ“Œ ê¸°ë³¸ ì›ì¹™

**âš ï¸ ì¤‘ìš”: ë¨¼ì € í˜„ì¬ êµ¬ì¡°ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì„ í™•ë³´í•˜ì„¸ìš”!**

ì„±ëŠ¥ ê°œì„ ì€ ë‹¤ìŒ ìˆœì„œë¡œ ì§„í–‰:
1. âœ… **í˜„ì¬ ResNet18ë¡œ í•™ìŠµ** â†’ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •
2. ğŸ“Š **ê²°ê³¼ ë¶„ì„** â†’ ì–´ë–¤ ë¬¸ì œê°€ ìˆëŠ”ì§€ íŒŒì•…
3. ğŸ”§ **íƒ€ê²Ÿ ê°œì„ ** â†’ ë¬¸ì œì— ë§ëŠ” í•´ê²°ì±… ì ìš©

---

### 1ï¸âƒ£ ëª¨ë¸ í¬ê¸° ì¦ê°€ (ì„±ëŠ¥ â†‘, ì†ë„ â†“)

#### Option A: ResNet50
**ì–¸ì œ ì‚¬ìš©:**
- ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì´ ë¶€ì¡±í•  ë•Œ
- ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ (5,000ì¥ ì´ìƒ âœ…)
- GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ë•Œ

**ë³€ê²½ ë°©ë²•:**

```yaml
# training/configs/cnn.yaml
model:
  name: resnet50  # resnet18 â†’ resnet50
  pretrained: true
  num_classes: 2
```

```python
# models/ct_cnn/model.py (í•´ë‹¹ ë¼ì¸ ìˆ˜ì •)
self.model = models.resnet50(pretrained=pretrained)
```

**ë¹„êµ:**

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ë ˆì´ì–´ | í•™ìŠµ ì‹œê°„ | ì„±ëŠ¥ |
|------|---------|--------|----------|------|
| ResNet18 | 11M | 18 | ê¸°ì¤€ | ê¸°ì¤€ |
| ResNet50 | 25M | 50 | ~2ë°° | +2~5% |
| ResNet101 | 44M | 101 | ~3ë°° | +3~7% |

**ì£¼ì˜ì‚¬í•­:**
- Batch sizeë¥¼ ì¤„ì—¬ì•¼ í•  ìˆ˜ ìˆìŒ (32 â†’ 16)
- í•™ìŠµ ì‹œê°„ ì¦ê°€
- ê³¼ì í•© ìœ„í—˜ (ë°ì´í„° ë¶€ì¡± ì‹œ)

---

#### Option B: EfficientNet (íš¨ìœ¨ì )
**ì¥ì :**
- ResNet50ë³´ë‹¤ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ ì„±ëŠ¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

**ë³€ê²½ ë°©ë²•:**

```python
# models/ct_cnn/model.py
import timm

class EfficientNetClassifier(nn.Module):
    def __init__(self, num_classes=2, pretrained=True):
        super().__init__()
        self.model = timm.create_model(
            'efficientnet_b0',
            pretrained=pretrained,
            num_classes=num_classes
        )
```

**ë¹„êµ:**

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ì„±ëŠ¥ | ì¶”ë¡  ì†ë„ |
|------|---------|------|----------|
| ResNet18 | 11M | ê¸°ì¤€ | ë¹ ë¦„ |
| EfficientNet-B0 | 5.3M | +3~5% | ë³´í†µ |
| EfficientNet-B1 | 7.8M | +4~7% | ë³´í†µ |

---

### 2ï¸âƒ£ Feature Map vs Vector ì´í•´

#### í˜„ì¬ êµ¬ì¡° (GAP ì‚¬ìš© - ì˜¬ë°”ë¦„!)

```python
Layer4 ì¶œë ¥: (512, 16, 16)  â† Feature Map (ê³µê°„ ì •ë³´ O)
    â†“
GAP (Global Average Pooling)
    â†“
Vector: (512)               â† ê³µê°„ ì •ë³´ X
    â†“
FC: (512) â†’ (2)
```

**Feature Map (2D):**
- í¬ê¸°: (ì±„ë„, ë†’ì´, ë„ˆë¹„) = (512, 16, 16)
- ê³µê°„ ì •ë³´ ìœ ì§€: "ì™¼ìª½ ìœ„ì— í¬ë™ì´ ìˆë‹¤"
- ìš©ë„: Object Detection, Segmentation

**Vector (1D):**
- í¬ê¸°: (ì±„ë„) = (512)
- ê³µê°„ ì •ë³´ ì—†ìŒ: "í¬ë™ì´ ìˆë‹¤" (ìœ„ì¹˜ ëª¨ë¦„)
- ìš©ë„: Classification (í˜„ì¬ ì‘ì—…!)

**ì™œ GAPê°€ ì¢‹ì€ê°€:**

```python
# GAP ì—†ì´ Flattenë§Œ ì‚¬ìš©í•˜ë©´:
Layer4: (512, 16, 16)
    â†“
Flatten: (512 Ã— 16 Ã— 16) = (131,072)  â† ë„ˆë¬´ í¼!
    â†“
FC: (131,072) â†’ (2)  â† íŒŒë¼ë¯¸í„° 262,144ê°œ (ë¹„íš¨ìœ¨!)

# GAP ì‚¬ìš© (í˜„ì¬):
Layer4: (512, 16, 16)
    â†“
GAP: ê° ì±„ë„ì˜ 16Ã—16 ê°’ì„ í‰ê·  â†’ (512)
    â†“
FC: (512) â†’ (2)  â† íŒŒë¼ë¯¸í„° 1,024ê°œ (íš¨ìœ¨ì !)
```

**ê²°ë¡ :**
- âœ… í˜„ì¬ ResNet18ì€ ì´ë¯¸ GAP ì‚¬ìš© ì¤‘
- âœ… Classificationì—ëŠ” GAPê°€ ìµœì 
- âœ… ë³€ê²½ ë¶ˆí•„ìš”!

---

### 3ï¸âƒ£ Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€ (ê³ ê¸‰)

**ì–¸ì œ ì‚¬ìš©:**
- ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì´ 80% ì´ìƒì¼ ë•Œ
- ë” ì •êµí•œ íŠ¹ì§• ì¶”ì¶œì´ í•„ìš”í•  ë•Œ
- "ì–´ë””ë¥¼ ë´ì•¼ í•˜ëŠ”ì§€" í•™ìŠµì‹œí‚¤ê³  ì‹¶ì„ ë•Œ

#### SE (Squeeze-and-Excitation) Block

```python
# models/ct_cnn/model.py
class SEBlock(nn.Module):
    """ì±„ë„ ê°„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)

    def forward(self, x):
        # x: (B, C, H, W)
        b, c, _, _ = x.size()

        # Global Average Pooling
        y = x.view(b, c, -1).mean(dim=2)  # (B, C)

        # Channel attention
        y = F.relu(self.fc1(y))
        y = torch.sigmoid(self.fc2(y))

        # Re-weight channels
        y = y.view(b, c, 1, 1)
        return x * y.expand_as(x)


class ResNetWithSE(nn.Module):
    """ResNet + SE Block"""
    def __init__(self, num_classes=2):
        super().__init__()
        self.resnet = models.resnet18(pretrained=True)

        # SE Blockì„ Layer4 ë’¤ì— ì¶”ê°€
        self.se = SEBlock(512)

        # FC layer êµì²´
        self.resnet.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.resnet.conv1(x)
        x = self.resnet.bn1(x)
        x = self.resnet.relu(x)
        x = self.resnet.maxpool(x)

        x = self.resnet.layer1(x)
        x = self.resnet.layer2(x)
        x = self.resnet.layer3(x)
        x = self.resnet.layer4(x)

        # SE Block ì ìš©
        x = self.se(x)  # â† ì¤‘ìš”í•œ ì±„ë„ì— ê°€ì¤‘ì¹˜

        x = self.resnet.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.resnet.fc(x)

        return x
```

**íš¨ê³¼:**
- ì¤‘ìš”í•œ íŠ¹ì§•ì— ì§‘ì¤‘
- ì•½ +1~3% ì„±ëŠ¥ í–¥ìƒ
- íŒŒë¼ë¯¸í„° ì¦ê°€ ê±°ì˜ ì—†ìŒ (~0.1M)

---

### 4ï¸âƒ£ Multi-Scale Features (ê³ ê¸‰)

**ê°œë…:**
- Layer2, Layer3, Layer4 ì¶œë ¥ì„ ëª¨ë‘ ì‚¬ìš©
- ì €ìˆ˜ì¤€ + ì¤‘ìˆ˜ì¤€ + ê³ ìˆ˜ì¤€ íŠ¹ì§• ê²°í•©

```python
class MultiScaleResNet(nn.Module):
    """ì—¬ëŸ¬ ë ˆì´ì–´ì˜ íŠ¹ì§•ì„ ê²°í•©"""
    def __init__(self, num_classes=2):
        super().__init__()
        resnet = models.resnet18(pretrained=True)

        # Backbone
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool

        self.layer1 = resnet.layer1  # (64, H/4, W/4)
        self.layer2 = resnet.layer2  # (128, H/8, W/8)
        self.layer3 = resnet.layer3  # (256, H/16, W/16)
        self.layer4 = resnet.layer4  # (512, H/32, W/32)

        # ê° ë ˆì´ì–´ë³„ GAP
        self.gap = nn.AdaptiveAvgPool2d(1)

        # ê²°í•© í›„ FC
        self.fc = nn.Linear(128 + 256 + 512, num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)

        # Layer2, 3, 4 íŠ¹ì§• ì¶”ì¶œ
        x2 = self.layer2(x)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)

        # ê°ê° GAP
        f2 = self.gap(x2).flatten(1)  # (B, 128)
        f3 = self.gap(x3).flatten(1)  # (B, 256)
        f4 = self.gap(x4).flatten(1)  # (B, 512)

        # Concatenate
        features = torch.cat([f2, f3, f4], dim=1)  # (B, 896)

        # Classification
        out = self.fc(features)
        return out
```

**íš¨ê³¼:**
- ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ê²°í•¨ ê°ì§€
- ë¯¸ì„¸í•œ í¬ë™ + í° ë³€í˜• ë™ì‹œ íƒì§€
- ì•½ +2~4% ì„±ëŠ¥ í–¥ìƒ

---

### 5ï¸âƒ£ Data Augmentation (ê°€ì¥ ë¨¼ì € ì‹œë„!)

**ì½”ìŠ¤íŠ¸ ì œë¡œ ì„±ëŠ¥ í–¥ìƒ!**

```python
# training/data/transforms.py
from torchvision import transforms

def get_train_transforms(image_size=512):
    """í•™ìŠµìš© Augmentation"""
    return transforms.Compose([
        transforms.Resize((image_size, image_size)),

        # ê¸°ë³¸ Augmentation
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),

        # ê³ ê¸‰ Augmentation
        transforms.ColorJitter(
            brightness=0.2,  # ë°ê¸° ë³€í™”
            contrast=0.2,    # ëŒ€ë¹„ ë³€í™”
            saturation=0.1   # ì±„ë„ ë³€í™”
        ),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.1, 0.1),  # ì´ë™
            scale=(0.9, 1.1)       # í¬ê¸° ë³€í™”
        ),

        # Normalize
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
```

**íš¨ê³¼:**
- ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€
- ê³¼ì í•© ë°©ì§€
- ì•½ +3~7% ì„±ëŠ¥ í–¥ìƒ
- **ì¶”ê°€ ë¹„ìš© ì—†ìŒ!**

---

### 6ï¸âƒ£ Learning Rate Scheduling

**í˜„ì¬ ë¬¸ì œ:**
- ê³ ì • Learning Rate (0.0001)
- í•™ìŠµ í›„ë°˜ë¶€ì— ë¯¸ì„¸ ì¡°ì • ì–´ë ¤ì›€

**í•´ê²°ì±…: Cosine Annealing**

```python
# models/ct_cnn/train.py
from torch.optim.lr_scheduler import CosineAnnealingLR

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)  # ì´ˆê¸° LR ë†’ì„

# Scheduler
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=config['training']['epochs'],
    eta_min=1e-6  # ìµœì†Œ LR
)

# í•™ìŠµ ë£¨í”„ì—ì„œ
for epoch in range(epochs):
    train_loss = train_epoch()
    val_loss, val_metrics = validate()

    # Scheduler step
    scheduler.step()
```

**íš¨ê³¼:**
- ì´ˆë°˜: ë¹ ë¥´ê²Œ ìˆ˜ë ´
- í›„ë°˜: ë¯¸ì„¸ ì¡°ì •
- ì•½ +1~3% ì„±ëŠ¥ í–¥ìƒ

---

### ğŸ“Š ê°œì„  ì „ëµ ìš°ì„ ìˆœìœ„

#### ğŸ¥‡ 1ìˆœìœ„ (ë¨¼ì € ì‹œë„)
1. **Data Augmentation** - ë¹„ìš© ì—†ì´ ì„±ëŠ¥ â†‘
2. **Learning Rate Scheduling** - ê°„ë‹¨í•œ ì½”ë“œ ì¶”ê°€
3. **Batch Size / Image Size ì¡°ì •** - Configë§Œ ë³€ê²½

#### ğŸ¥ˆ 2ìˆœìœ„ (ë² ì´ìŠ¤ë¼ì¸ 70% ì´ìƒì¼ ë•Œ)
4. **ResNet50ìœ¼ë¡œ ë³€ê²½** - ëª¨ë¸ í¬ê¸° ì¦ê°€
5. **EfficientNet ì‹œë„** - íš¨ìœ¨ì ì¸ ëª¨ë¸

#### ğŸ¥‰ 3ìˆœìœ„ (ë² ì´ìŠ¤ë¼ì¸ 80% ì´ìƒì¼ ë•Œ)
6. **SE Block ì¶”ê°€** - Attention ë©”ì»¤ë‹ˆì¦˜
7. **Multi-Scale Features** - ë³µì¡í•œ êµ¬ì¡° ë³€ê²½

---

### âš ï¸ ì£¼ì˜ì‚¬í•­

**í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:**
- âŒ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì„  ë™ì‹œ ì ìš© â†’ ì–´ë–¤ ê²Œ íš¨ê³¼ì ì¸ì§€ ëª¨ë¦„
- âŒ ë² ì´ìŠ¤ë¼ì¸ ì—†ì´ ë³µì¡í•œ ëª¨ë¸ë¶€í„° ì‹œì‘
- âŒ ë°ì´í„° ë¶„ì„ ì—†ì´ ë¬´ì‘ì • ëª¨ë¸ë§Œ ë³€ê²½

**í•´ì•¼ í•  ê²ƒ:**
- âœ… í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½
- âœ… ê° ë³€ê²½ì˜ íš¨ê³¼ ì¸¡ì •
- âœ… ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡ (Config + ì„±ëŠ¥)

---

### ğŸ“ˆ ì‹¤í—˜ ë¡œê·¸ ì˜ˆì‹œ

```markdown
## ì‹¤í—˜ ê¸°ë¡

### Baseline (ResNet18)
- Config: image_size=512, lr=0.0001, bs=32
- ê²°ê³¼: F1=0.78, Acc=0.80
- ë¬¸ì œ: Recallì´ ë‚®ìŒ (ë¶ˆëŸ‰ ë¯¸íƒ ë§ìŒ)

### Experiment 1: Data Augmentation
- ë³€ê²½: RandomFlip, Rotation, ColorJitter ì¶”ê°€
- ê²°ê³¼: F1=0.82 (+4%), Acc=0.83
- íš¨ê³¼: âœ… ê³¼ì í•© ê°ì†Œ, Recall ê°œì„ 

### Experiment 2: ResNet50
- ë³€ê²½: ResNet18 â†’ ResNet50
- ê²°ê³¼: F1=0.84 (+2%), Acc=0.85
- íš¨ê³¼: âœ… ë¯¸ì„¸í•œ ê²°í•¨ íƒì§€ ê°œì„ 
- ë¹„ìš©: í•™ìŠµ ì‹œê°„ 2ë°° ì¦ê°€

### Experiment 3: SE Block
- ë³€ê²½: Layer4 ë’¤ì— SE Block ì¶”ê°€
- ê²°ê³¼: F1=0.85 (+1%), Acc=0.86
- íš¨ê³¼: âœ… ì¤‘ìš”í•œ íŠ¹ì§•ì— ì§‘ì¤‘
- ë¹„ìš©: íŒŒë¼ë¯¸í„° ê±°ì˜ ì¦ê°€ ì—†ìŒ
```

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2024-12-29
**ìµœì¢… ìˆ˜ì •ì¼**: 2024-12-29
**ëª¨ë¸ ë²„ì „**: ResNet18 (ImageNet Pretrained)
**í”„ë¡œì íŠ¸**: ë°°í„°ë¦¬ ë¶ˆëŸ‰ ê²€ì‚¬ ì‹œìŠ¤í…œ
